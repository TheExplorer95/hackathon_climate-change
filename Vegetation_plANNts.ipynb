{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a timer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    \"\"\"\n",
    "    A small class to measure time during training.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start a new timer\n",
    "        \"\"\"\n",
    "        self._start_time = time.perf_counter()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"\n",
    "        Stop the timer, and report the elapsed time\n",
    "        \"\"\"\n",
    "        if self._start_time is None:\n",
    "            print(f\"Timer is not running. Use .start() to start it\")\n",
    "            return 0\n",
    "    \n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        self._start_time = None\n",
    "        \n",
    "        return elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data param\n",
    "day_len = 30\n",
    "batch_size = 16\n",
    "long = 36\n",
    "lat = 72\n",
    "\n",
    "# model param\n",
    "num_filters = 16\n",
    "input_shape = (batch_size, day_len, long, lat, 2)\n",
    "\n",
    "# training param\n",
    "epochs = 3\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "data_models = ['GFDL-ESM4','IPSL-CM6A-LR','MPI-ESM1-2-HR']  # models for temp, prec, LAI\n",
    "dmodel = 'IPSL-CM6A-LR'   # TODO: choose a model\n",
    "\n",
    "temp_ds = np.array(xr.open_mfdataset('data/near_surface_air_temperature/historical/{}/*.nc'.format(dmodel)).tas)\n",
    "\n",
    "prec_ds = np.array(xr.open_mfdataset('data/precipitation_flux/historical/{}/*.nc'.format(dmodel)).pr)\n",
    "\n",
    "npp_files = glob.glob('data/net_primary_production_on_land/historical/**/*.nc', recursive=True) # TODO: use all models\n",
    "npp_ds = np.array(xr.open_mfdataset(np.random.choice(np.array(npp_files))).npp)\n",
    "npp_ds = np.nan_to_num(npp_ds)\n",
    "\n",
    "lai_dx = xr.open_mfdataset('data/leaf_area_index/historical/{}/*.nc'.format(dmodel))  # not numpy as time index needed\n",
    "lai_dx['lai'] = lai_dx.lai.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the outputs (LAI and NPP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max scale outputs to [0,10]\n",
    "max_output = 10\n",
    "\n",
    "npp_ds_s = max_output* (npp_ds-np.min(npp_ds))/(np.max(npp_ds)-np.min(npp_ds))\n",
    "\n",
    "lai_ds_o = lai_dx['lai'] \n",
    "lai_dx[\"lai\"] = max_output * (lai_dx.lai - np.min(lai_dx.lai))/(np.max(lai_dx.lai)-np.min(lai_dx.lai))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_month = 1978-(12*14)\n",
    "train_min_month = day_len//28\n",
    "\n",
    "val_max_month = 1978\n",
    "val_min_month = (day_len//28) + (1978-(12*14))\n",
    "\n",
    "# min-max scale inputs normalized only on train data (statistics will also be used to normalize test set)\n",
    "train_max_index = temp_ds.shape[0] - (365*14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temp = np.min(temp_ds)\n",
    "max_temp = np.max(temp_ds)\n",
    "\n",
    "min_prec = np.min(prec_ds)\n",
    "max_prec = np.max(prec_ds)\n",
    "\n",
    "min_input= -1\n",
    "max_input = 1\n",
    "\n",
    "temp_ds_s = (temp_ds - min_temp) * (max_input - min_input) / (max_temp - min_temp)\n",
    "                                  \n",
    "prec_ds_s = (prec_ds - min_prec) * (max_input - min_input) / (max_prec - min_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of output month indicies needed for plotting and generator\n",
    "val_months = 12*14 # 14 years of evaluation data\n",
    "\n",
    "train_max_month = 1978-val_months\n",
    "min_month = day_len//28\n",
    "\n",
    "val_max_month = 1978\n",
    "val_min_month = train_max_month +1\n",
    "\n",
    "def train_gen_data_card():\n",
    "    while True:\n",
    "        # array to append to\n",
    "        endstamp = []\n",
    "        output_day_i = np.zeros(batch_size)\n",
    "        \n",
    "        lai = np.zeros((batch_size, npp_ds.shape[1], npp_ds.shape[2]))  # batch, lon, lat\n",
    "        npp = np.zeros((batch_size, npp_ds.shape[1], npp_ds.shape[2]))\n",
    "        temp = np.zeros((batch_size, day_len, npp_ds.shape[1], npp_ds.shape[2]))  # batch, time, lon, lat\n",
    "        prec = np.zeros((batch_size, day_len, npp_ds.shape[1], npp_ds.shape[2]))\n",
    "        \n",
    "        # index of output in month\n",
    "        output_month_i = np.random.randint(train_min_month, train_max_month, size=batch_size)  # y_pred timepoint in int\n",
    "\n",
    "        # convert output index to timestamp\n",
    "        try:\n",
    "            for i in range(batch_size):\n",
    "                endstamp.append(lai_dx.indexes['time'].to_datetimeindex()[output_month_i[i]])  # cfttimeindex to datetime               \n",
    "        except:\n",
    "            for i in range(batch_size):\n",
    "                endstamp.append(lai_dx.indexes['time'][output_month_i[i]])\n",
    "\n",
    "        # convert output month index to day index\n",
    "        for i in range(batch_size):\n",
    "            output_day_i[i] = (endstamp[i] - pd.Timestamp('1850-01-01T12')).days  # output is i-th day in int\n",
    "        output_day_i = np.int_(output_day_i)\n",
    "\n",
    "        # save month-based time slices\n",
    "        lainp = np.array(lai_dx.lai)\n",
    "        for i in range(batch_size):\n",
    "            lai[i] = lainp[output_month_i[i]]\n",
    "        for i in range(batch_size):\n",
    "            npp[i] = npp_ds_s[output_month_i[i]]\n",
    "\n",
    "        # day-based metrics\n",
    "        for i in range(batch_size):\n",
    "            temp[i] = temp_ds_s[output_day_i[i]-day_len:output_day_i[i]]\n",
    "        for i in range(batch_size):\n",
    "            prec[i] = prec_ds_s[output_day_i[i]-day_len:output_day_i[i]]\n",
    "\n",
    "        # merge features\n",
    "        inputs = np.stack((temp,prec), axis=-1)  # two features\n",
    "        outputs = np.stack((lai,npp), axis=-1)\n",
    "\n",
    "        yield (inputs, outputs)\n",
    "        \n",
    "def val_gen_data_card():\n",
    "    while True:\n",
    "        # array to append to\n",
    "        endstamp = []\n",
    "        output_day_i = np.zeros(batch_size)\n",
    "        \n",
    "        lai = np.zeros((batch_size, npp_ds.shape[1], npp_ds.shape[2]))  # batch, lon, lat\n",
    "        npp = np.zeros((batch_size, npp_ds.shape[1], npp_ds.shape[2]))\n",
    "        temp = np.zeros((batch_size, day_len, npp_ds.shape[1], npp_ds.shape[2]))  # batch, time, lon, lat\n",
    "        prec = np.zeros((batch_size, day_len, npp_ds.shape[1], npp_ds.shape[2]))\n",
    "        \n",
    "        # index of output in month\n",
    "        output_month_i = np.random.randint(val_min_month, val_max_month, size=batch_size)  # y_pred timepoint in int\n",
    "\n",
    "        # convert output index to timestamp\n",
    "        try:\n",
    "            for i in range(batch_size):\n",
    "                endstamp.append(lai_dx.indexes['time'].to_datetimeindex()[output_month_i[i]])  # cfttimeindex to datetime               \n",
    "        except:\n",
    "            for i in range(batch_size):\n",
    "                endstamp.append(lai_dx.indexes['time'][output_month_i[i]])\n",
    "\n",
    "        # convert output month index to day index\n",
    "        for i in range(batch_size):\n",
    "            output_day_i[i] = (endstamp[i] - pd.Timestamp('1850-01-01T12')).days  # output is i-th day in int\n",
    "        output_day_i = np.int_(output_day_i)\n",
    "\n",
    "        # save month-based time slices\n",
    "        lainp = np.array(lai_dx.lai)\n",
    "        for i in range(batch_size):\n",
    "            lai[i] = lainp[output_month_i[i]]\n",
    "        for i in range(batch_size):\n",
    "            npp[i] = npp_ds_s[output_month_i[i]]\n",
    "\n",
    "        # day-based metrics\n",
    "        for i in range(batch_size):\n",
    "            temp[i] = temp_ds_s[output_day_i[i]-day_len:output_day_i[i]]\n",
    "        for i in range(batch_size):\n",
    "            prec[i] = prec_ds_s[output_day_i[i]-day_len:output_day_i[i]]\n",
    "\n",
    "        # merge features\n",
    "        inputs = np.stack((temp,prec), axis=-1)  # two features\n",
    "        outputs = np.stack((lai,npp), axis=-1)\n",
    "\n",
    "        yield (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_generator(train_gen_data_card, output_types = (tf.float32,tf.float32))\n",
    "train_ds = train_ds.take(30).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(val_gen_data_card, output_types = (tf.float32,tf.float32))\n",
    "val_ds = val_ds.take(30).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Convolutional LSTM-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        do = 0.2\n",
    "        self.convlstm2D_1 = tf.keras.layers.ConvLSTM2D(filters=num_filters,\n",
    "                                                       kernel_size=(3,3),\n",
    "                                                       padding=\"same\",\n",
    "                                                       return_sequences=True,\n",
    "                                                       activation=\"tanh\")\n",
    "        self.bn_1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.convlstm2D_2 = tf.keras.layers.ConvLSTM2D(filters=num_filters,\n",
    "                                                       kernel_size=(3,3),\n",
    "                                                       padding=\"same\",\n",
    "                                                       return_sequences=True,\n",
    "                                                       activation=\"tanh\")\n",
    "        self.bn_2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "\n",
    "        self.convlstm2D_3 = tf.keras.layers.ConvLSTM2D(filters=num_filters,\n",
    "                                                       kernel_size=(3,3),\n",
    "                                                       padding=\"same\",\n",
    "                                                       return_sequences=True,\n",
    "                                                       activation=\"tanh\")\n",
    "        self.bn_3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.convlstm2D_4 = tf.keras.layers.ConvLSTM2D(filters=num_filters,\n",
    "                                                       kernel_size=(3,3),\n",
    "                                                       padding=\"same\",\n",
    "                                                       return_sequences=True,\n",
    "                                                       activation=\"tanh\")\n",
    "        self.bn_4 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # convolve over time, lat, lon. This means that we assume timesteps close to each other share local similarities\n",
    "        self.conv3d = tf.keras.layers.Conv3D(filters=2,\n",
    "                                             kernel_size=(3,3,3), \n",
    "                                             activation=\"tanh\",\n",
    "                                             padding=\"same\")\n",
    "        \n",
    "        self.do3d = tf.keras.layers.Dropout(do)\n",
    "        # computed convolved sum over all time dimension to get a single time slice\n",
    "        self.bottleneck = tf.keras.layers.Conv3D(filters=1,\n",
    "                                                 kernel_size=1,\n",
    "                                                 activation=\"relu\",\n",
    "                                                 strides=1)\n",
    "\n",
    "\n",
    "    def call(self, x, training, input_shape):\n",
    "        # (batch, time, lat, lon, channel)\n",
    "        x = tf.ensure_shape(x, input_shape) \n",
    "        # (batch, time, lat, lon, channel)\n",
    "        x = self.convlstm2D_1(x,training= training)\n",
    "        # (batch, time, lat1, lon1, filter1)\n",
    "        x = self.bn_1(x,training = training)\n",
    "        \n",
    "        x = self.convlstm2D_2(x,training = training)\n",
    "        x = self.bn_2(x,training = training)\n",
    "        \n",
    "        x = self.convlstm2D_3(x,training = training)\n",
    "        x = self.bn_3(x,training = training)\n",
    "        \n",
    "        x = self.convlstm2D_4(x,training = training)\n",
    "        x = self.bn_4(x, training = training)\n",
    "        # (batch, time, lat4, lon4, filter4)\n",
    "        x = self.do3d(x,training= training)\n",
    "        x = self.conv3d(x)\n",
    "        # (batch, newtime, newlat, newlon, newfilter=2)\n",
    "        \n",
    "        x = tf.transpose(x, [0,4,2,3,1])\n",
    "        # (batch, 2, lat, lon, time)\n",
    "        x = self.bottleneck(x)\n",
    "        # (batch, 2, lat, lon, 1)\n",
    "        \n",
    "        x = tf.transpose(x, [0,4,2,3,1])\n",
    "        # (batch, 1, lat, lon, 2)\n",
    "        x = tf.squeeze(x,axis=1)\n",
    "        # (batch, lat, lon, 2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a custom train and evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, data, loss_function, optimizer, train_loss_metric, input_shape=input_shape):\n",
    "    '''\n",
    "    Training for one epoch.\n",
    "    '''\n",
    "    for img, target in train_ds:\n",
    "        # forward pass with GradientTape\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = model(img, training=True, input_shape=input_shape)\n",
    "            loss = loss_function(target, prediction) + tf.reduce_sum(model.losses)\n",
    "\n",
    "        # backward pass via GradienTape (auto-gradient calc)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        train_loss_metric.update_state(loss)\n",
    "        \n",
    "#@tf.function        \n",
    "def eval_step(model, ds, loss_function, loss_metric, input_shape=input_shape):\n",
    "    '''\n",
    "    Evaluation Loop.\n",
    "    '''\n",
    "    for sequence, target in ds:\n",
    "        # forward pass\n",
    "        prediction = model(sequence, training=False, input_shape=input_shape)\n",
    "        # update metrics\n",
    "        loss = loss_function(target, prediction)\n",
    "        loss_metric.update_state(loss)\n",
    "        \n",
    "    fig, axe = plt.subplots(2,3, figsize=(25,10))\n",
    "    plt.tight_layout(pad= 0.05)\n",
    "    axe[0,0].imshow(target[0,:,:,0], cmap='gray', origin='lower')\n",
    "    axe[0,1].imshow(prediction[0,:,:,0], cmap='gray', origin='lower')\n",
    "    axe[1,0].imshow(target[0,:,:,1], cmap='gray', origin='lower')\n",
    "    axe[1,1].imshow(prediction[0,:,:,1], cmap='gray', origin='lower')\n",
    "    axe[0,2].imshow(np.abs(target[0,:,:,0]-prediction[0,:,:,0]), cmap = \"bwr\", \n",
    "                    origin= \"lower\", vmin = 0,vmax =max_output)\n",
    "    axe[1,2].imshow(np.abs(target[0,:,:,1]-prediction[0,:,:,1]), cmap = \"bwr\", \n",
    "                    origin= \"lower\", vmin = 0,vmax =max_output)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model, Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvLSTM(num_filters=num_filters)\n",
    "\n",
    "# define loss\n",
    "loss_function = tf.keras.losses.MSE\n",
    "# define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating Tensorboard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = Timer()\n",
    "times = []\n",
    "\n",
    "# 2nd-order metric to take mean over all samples\n",
    "train_loss_metric = tf.keras.metrics.Mean('train_loss')\n",
    "val_loss_metric = tf.keras.metrics.Mean('val_loss')\n",
    "\n",
    "# initialize the logger for Tensorboard visualization\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train_ConvLSTM'    \n",
    "val_log_dir = 'logs/gradient_tape/' + current_time + '/val_ConvLSTM'       \n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)  \n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating over the epochs (finally training the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'\\n[EPOCH] ____________________{epoch}____________________')\n",
    "    \n",
    "    # training step with metrics update--------------------------------------------------------\n",
    "    timer.start()\n",
    "    \n",
    "    train_step(model, train_ds, loss_function, optimizer, train_loss_metric)\n",
    "    \n",
    "    train_loss = train_loss_metric.result()\n",
    "    with train_summary_writer.as_default():     # logging our metrics to a file which is used by tensorboard\n",
    "        tf.summary.scalar('loss', train_loss, step=epoch)\n",
    "    elapsed_time = timer.stop()\n",
    "    \n",
    "    print(f'[{epoch}] - Finished Epoch in {elapsed_time:0.2f} seconds - train_loss: {train_loss:0.4f}')\n",
    "    \n",
    "    # evaluation step with metrics update--------------------------------------------------------\n",
    "    timer.start()\n",
    "    \n",
    "    eval_step(model, val_ds, loss_function, loss_metric=val_loss_metric)\n",
    "    \n",
    "    val_loss = val_loss_metric.result()    \n",
    "    with val_summary_writer.as_default():       # logging our metrics to a file which is used by tensorboard\n",
    "        tf.summary.scalar('loss', val_loss, step=epoch)    \n",
    "    \n",
    "    print(f'[{epoch}] - Finished evaluation - val_loss: {val_loss:0.4f}')\n",
    "    \n",
    "    # Resetting train and validation metrics-----------------------------------------------------\n",
    "    train_loss_metric.reset_states()\n",
    "    val_loss_metric.reset_states()\n",
    "    elapsed_time = timer.stop()\n",
    "    times.append(elapsed_time)\n",
    "  \n",
    "    if epoch%3 == 0:\n",
    "        print(f'\\n[INFO] - Total time elapsed: {np.sum(times)/60:0.4f} min. Total time remaining: {(np.sum(times)/(epoch+1))*(epochs-epoch-1)/60:0.4f} min.')\n",
    "\n",
    "print(f'\\n[INFO] - Total run time: {np.sum(times)/60:0.4f} min.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_name = \"ConvLSTM_02dropout_16filters\"\n",
    "\n",
    "model.save_weights(\"model_weights/\"+ training_name + \".hdf5\",save_format=\"hdf5\",overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the future of plants with our trained model\n",
    "\n",
    "It is to note that we trained the model on the full dataset again after evaluation (including the most recent 14 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_models = ['GFDL-ESM4','IPSL-CM6A-LR','MPI-ESM1-2-HR']  # models for temp, prec, LAI\n",
    "dmodel = 'IPSL-CM6A-LR' \n",
    "scenarios = ['ssp126', 'ssp370', 'ssp585']\n",
    "\n",
    "scenario = scenarios[0] # 0 optimistic, 1 intermediate, 2 pessimistic\n",
    "\n",
    "# load historical data\n",
    "dx_temp = xr.open_mfdataset('data/near_surface_air_temperature/historical/{}/*.nc'.format(dmodel)).tas\n",
    "dx_prec = xr.open_mfdataset('data/precipitation_flux/historical/{}/*.nc'.format(dmodel)).pr\n",
    "\n",
    "# load prediction of climate until 2100 and concatanate with historical data\n",
    "dx_temp_future = xr.open_mfdataset('data/near_surface_air_temperature/{}/{}/*.nc'.format(scenario, dmodel)).tas\n",
    "dx_temp_future = xr.concat((dx_temp, dx_temp_future), dim='time')\n",
    "dx_prec_future = xr.open_mfdataset('data/precipitation_flux/{}/{}/*.nc'.format(scenario, dmodel)).pr\n",
    "dx_prec_future = xr.concat((dx_prec, dx_prec_future), dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the future's input data based on the past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "train_max_index = dx_temp.shape[0] - (365*14)\n",
    "\n",
    "min_temp = np.min(dx_temp)\n",
    "max_temp = np.max(dx_temp)\n",
    "\n",
    "min_prec = np.min(dx_prec)\n",
    "max_prec = np.max(dx_prec)\n",
    "\n",
    "min_input = -1\n",
    "max_input = 1\n",
    "\n",
    "dx_temp_future = (dx_temp_future - min_temp) * (max_input - min_input) / (max_temp - min_temp)\n",
    "dx_prec_future = (dx_prec_future - min_prec) * (max_input - min_input) / (max_prec - min_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Dataset input pipeline for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_months = (2100-2015+1)*12\n",
    "\n",
    "def gen_future_climate():\n",
    "#     first_month = (2015-1850)*12 + 1  # int index of which month january 2015 is with 0 being january 1850\n",
    "\n",
    "    counter = 0\n",
    "    while counter < total_months: # predict from 2015 to 2100, excluding last month\n",
    "        cyear = (counter//12) + 2015\n",
    "        \n",
    "        cmonth = (counter+1) % 12\n",
    "        \n",
    "        if cmonth == 0:\n",
    "            cmonth = 12\n",
    "        \n",
    "        current_timestamp = pd.Timestamp(cyear, cmonth, 1)\n",
    "        input_start_timestamp = current_timestamp - pd.Timedelta(day_len-1, unit='day')\n",
    "\n",
    "        counter += 1\n",
    "       \n",
    "        data = np.stack((np.array(dx_temp_future.loc[input_start_timestamp:current_timestamp+pd.Timedelta(1,unit='day')]),\n",
    "                         np.array(dx_prec_future.loc[input_start_timestamp:current_timestamp+pd.Timedelta(1,unit='day')])),\n",
    "                         axis=-1)\n",
    "        \n",
    "        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pipeline\n",
    "ds_future = tf.data.Dataset.from_generator(generator=gen_future_climate, \n",
    "                                           output_types=(tf.float32)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvLSTM(num_filters=num_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load weights of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, weight_file, dataset, input_shape):\n",
    "    \"\"\"\n",
    "    Builds the model by using the call method on input and then loads the weights.\n",
    "    \"\"\"\n",
    "    for data in dataset.take(1):\n",
    "        model(data, training=False, input_shape=input_shape)\n",
    "    \n",
    "    model.load_weights(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = 'ConvLSTM_02dropout_16filters'\n",
    "\n",
    "load_weights(model, f'model_weights/{weight_file}.hdf5', ds_future, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting 2015-2100 under the previously specified scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for data in ds_future:\n",
    "    \n",
    "    try:\n",
    "        predictions = np.concatenate((predictions, model(data, training=False, input_shape=input_shape)), axis=0)    \n",
    "    except Exception as e:\n",
    "        predictions = model(data, training=False, input_shape=input_shape)\n",
    "    \n",
    "    print(f'[INFO] - {counter/int(total_months/batch_size)*100:.2f}% finished')\n",
    "    counter += 1\n",
    "\n",
    "print('[INFO] - Done')\n",
    "\n",
    "np.save('results/pred_{}_{}'.format(weight_file, scenario), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"animation.html\"] = \"jshtml\"  # allow animation for jupyter\n",
    "plt.rcParams['xtick.bottom'] = False\n",
    "plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['ytick.labelleft'] = False\n",
    "plt.rcParams.update({'font.size': 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictions\n",
    "pred_p = np.load('results/pred_Dropout_historic_Pessimistic.npy')\n",
    "pred_m = np.load('results/pred_Dropout_historic_Medium.npy')\n",
    "pred_o = np.load('results/pred_Dropout_historic_Optimistic.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animation of LAI prediction based on pessimistic temp and precipitation data (also suited for NPP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "graph = 'NPP' # change to NPP for NPP animation\n",
    "\n",
    "frames = []  # append each image\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "\n",
    "for timeindex in range(total_months):  # animate for 1 yr\n",
    "\n",
    "    frames.append([plt.imshow(pred_p[timeindex,:,:,0 if graph == 'LAI' else 1],  # change 0 to 1 for NPP\n",
    "                              cmap='gray', origin='lower', animated=True)])\n",
    "\n",
    "ani = matplotlib.animation.ArtistAnimation(fig, frames, interval=200, blit=True, repeat=False)\n",
    "# ani.save('figs/pred_drop_Pessimistic_LAI.gif', writer='imagemagick', fps=10)\n",
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot general tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load history\n",
    "dn_lai = np.array(xr.open_dataset('data/leaf_area_index/historical/IPSL-CM6A-LR/lai_Lmon_IPSL-CM6A-LR_historical_r1i1p1f1_185001-201412.nc').lai)\n",
    "dn_lai = np.nan_to_num(dn_lai)\n",
    "min_lai = np.min(dn_lai)\n",
    "max_lai = np.max(dn_lai)\n",
    "dn_npp = np.array(xr.open_dataset('data/net_primary_production_on_land/historical/CMCC-CM2-SR5/npp_Lmon_CMCC-CM2-SR5_historical_r1i1p1f1_185001-201412.nc').npp)\n",
    "dn_npp = np.nan_to_num(dn_npp)\n",
    "min_npp = np.min(dn_npp)\n",
    "max_npp = np.max(dn_npp)\n",
    "\n",
    "# re scale pred\n",
    "def redo_minmax(data, min_data, max_data, max_output):\n",
    "    return ((data*(max_data-min_data))/max_output)+min_data\n",
    "\n",
    "max_output = 10\n",
    "lai_future_p = redo_minmax(pred_p[:,:,:,0], min_lai, max_lai, max_output)\n",
    "npp_future_p = redo_minmax(pred_p[:,:,:,1], min_npp, max_npp, max_output)\n",
    "lai_future_m = redo_minmax(pred_m[:,:,:,0], min_lai, max_lai, max_output)\n",
    "npp_future_m = redo_minmax(pred_m[:,:,:,1], min_npp, max_npp, max_output)\n",
    "lai_future_o = redo_minmax(pred_o[:,:,:,0], min_lai, max_lai, max_output)\n",
    "npp_future_o = redo_minmax(pred_o[:,:,:,1], min_npp, max_npp, max_output)\n",
    "\n",
    "# compute mean\n",
    "lai_future_p = np.mean(lai_future_p,axis=(1,2))\n",
    "npp_future_p = np.mean(npp_future_p,axis=(1,2))\n",
    "lai_future_m = np.mean(lai_future_m,axis=(1,2))\n",
    "npp_future_m = np.mean(npp_future_m,axis=(1,2))\n",
    "lai_future_o = np.mean(lai_future_o,axis=(1,2))\n",
    "npp_future_o = np.mean(npp_future_o,axis=(1,2))\n",
    "dn_lai = np.mean(dn_lai,axis=(1,2))\n",
    "dn_npp = np.mean(dn_npp,axis=(1,2))\n",
    "\n",
    "# load climatic data\n",
    "dn_temp = np.mean(xr.open_mfdataset('data/near_surface_air_temperature/historical/IPSL-CM6A-LR/*.nc').tas[::31], axis=(1,2))\n",
    "dn_temp_p = np.mean(xr.open_mfdataset('data/near_surface_air_temperature/ssp585/IPSL-CM6A-LR/*.nc').tas[::31], axis=(1,2))\n",
    "dn_temp_m = np.mean(xr.open_mfdataset('data/near_surface_air_temperature/ssp370/IPSL-CM6A-LR/*.nc').tas[::31], axis=(1,2))\n",
    "dn_temp_o = np.mean(xr.open_mfdataset('data/near_surface_air_temperature/ssp126/IPSL-CM6A-LR/*.nc').tas[::31], axis=(1,2))\n",
    "\n",
    "dn_prec = np.mean(xr.open_mfdataset('data/precipitation_flux/historical/IPSL-CM6A-LR/*.nc').pr[::31], axis=(1,2))\n",
    "dn_prec_p = np.mean(xr.open_mfdataset('data/precipitation_flux/ssp585/IPSL-CM6A-LR/*.nc').pr[::31], axis=(1,2))\n",
    "dn_prec_m = np.mean(xr.open_mfdataset('data/precipitation_flux/ssp370/IPSL-CM6A-LR/*.nc').pr[::31], axis=(1,2))\n",
    "dn_prec_o = np.mean(xr.open_mfdataset('data/precipitation_flux/ssp126/IPSL-CM6A-LR/*.nc').pr[::31], axis=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artifacts within the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted future values are shifthed and also quite noisy. To deal with that, we have adjusted the offset and also used a gaussian filter to reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(40,5))\n",
    "plt.plot(np.arange(dn_lai.shape[0]),\n",
    "         dn_lai,\n",
    "         color='gray')\n",
    "plt.plot(np.arange(dn_lai.shape[0], dn_lai.shape[0]+lai_future_p.shape[0]), \n",
    "         lai_future_p,\n",
    "         color='red', linestyle='dotted', alpha=0.5)\n",
    "plt.plot(np.arange(dn_lai.shape[0], dn_lai.shape[0]+lai_future_p.shape[0]), \n",
    "         lai_future_m,\n",
    "         color='blue', linestyle='dotted', alpha=0.5)\n",
    "plt.plot(np.arange(dn_lai.shape[0], dn_lai.shape[0]+lai_future_p.shape[0]), \n",
    "         lai_future_o,\n",
    "         color='green', linestyle='dotted', alpha=0.5)\n",
    "ax.set(xlabel='time (month)', ylabel='normalized LAI')\n",
    "\n",
    "# fig.savefig('figs/artifact_scaling.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot with all variables, takes mean of one entire LAI or NPP image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 10\n",
    "offset_lai = 0.05\n",
    "offset_npp = -2.48*1e-8\n",
    "alpha = 0.8\n",
    "\n",
    "fig,ax = plt.subplots(4,1, figsize=(40,25), sharex=True)\n",
    "\n",
    "ax[0].plot(np.arange(dn_lai.shape[0]),\n",
    "         gaussian_filter1d(dn_lai,sigma),\n",
    "         color='gray')\n",
    "ax[0].plot(np.arange(dn_lai.shape[0], dn_lai.shape[0]+lai_future_p.shape[0]), \n",
    "         gaussian_filter1d(lai_future_p,sigma) + offset_lai,\n",
    "         color='red', linestyle='dotted', alpha=alpha, label='pessimistic (ssp585)')\n",
    "ax[0].plot(np.arange(dn_lai.shape[0], dn_lai.shape[0]+lai_future_p.shape[0]), \n",
    "         gaussian_filter1d(lai_future_m,sigma) + offset_lai,\n",
    "         color='blue', linestyle='dotted', alpha=alpha, label='intermediate (ssp370)')\n",
    "ax[0].plot(np.arange(dn_lai.shape[0], dn_lai.shape[0]+lai_future_p.shape[0]), \n",
    "         gaussian_filter1d(lai_future_o,sigma) + offset_lai,\n",
    "         color='green', linestyle='dotted', alpha=alpha, label='optimistic (ssp126)')\n",
    "ax[0].set(ylabel='normalized LAI')\n",
    "\n",
    "ax[1].plot(np.arange(dn_lai.shape[0]),\n",
    "         gaussian_filter1d(dn_npp,sigma),\n",
    "         color='gray')\n",
    "ax[1].plot(np.arange(dn_lai.shape[0], dn_lai.shape[0]+lai_future_p.shape[0]), \n",
    "         gaussian_filter1d(npp_future_p,sigma) + offset_npp,\n",
    "         color='red', linestyle='dotted', alpha=alpha, label='pessimistic (ssp585)')\n",
    "ax[1].plot(np.arange(dn_lai.shape[0], dn_lai.shape[0]+lai_future_p.shape[0]), \n",
    "         gaussian_filter1d(npp_future_m,sigma) + offset_npp,\n",
    "         color='blue', linestyle='dotted', alpha=alpha, label='intermediate (ssp370)')\n",
    "ax[1].plot(np.arange(dn_lai.shape[0], dn_lai.shape[0]+lai_future_p.shape[0]), \n",
    "         gaussian_filter1d(npp_future_o,sigma) + offset_npp,\n",
    "         color='green', linestyle='dotted', alpha=alpha, label='optimistic (ssp126)')\n",
    "ax[1].set( ylabel='normalized NPP')\n",
    "\n",
    "\n",
    "ax[2].plot(np.arange(dn_temp.shape[0]),\n",
    "         gaussian_filter1d(dn_temp,sigma),\n",
    "         color='gray')\n",
    "ax[2].plot(np.arange(dn_temp.shape[0], dn_temp.shape[0]+dn_temp_p.shape[0]),\n",
    "         gaussian_filter1d(dn_temp_p,sigma),\n",
    "         color='red', linestyle='dotted', alpha=alpha, label='pessimistic (ssp585)')\n",
    "ax[2].plot(np.arange(dn_temp.shape[0], dn_temp.shape[0]+dn_temp_p.shape[0]),\n",
    "         gaussian_filter1d(dn_temp_m,sigma),\n",
    "         color='blue', linestyle='dotted', alpha=alpha, label='intermediate (ssp370)')\n",
    "ax[2].plot(np.arange(dn_temp.shape[0], dn_temp.shape[0]+dn_temp_p.shape[0]),\n",
    "         gaussian_filter1d(dn_temp_o,sigma),\n",
    "         color='green', linestyle='dotted', alpha=alpha, label='optimistic (ssp126)')\n",
    "ax[2].set(ylabel='temperature')\n",
    "\n",
    "ax[3].plot(np.arange(dn_temp.shape[0]),\n",
    "         gaussian_filter1d(dn_prec,sigma),\n",
    "         color='gray')\n",
    "ax[3].plot(np.arange(dn_temp.shape[0], dn_temp.shape[0]+dn_temp_p.shape[0]),\n",
    "         gaussian_filter1d(dn_prec_p,sigma),\n",
    "         color='red', linestyle='dotted', alpha=alpha, label='pessimistic (ssp585)')\n",
    "ax[3].plot(np.arange(dn_temp.shape[0], dn_temp.shape[0]+dn_temp_p.shape[0]),\n",
    "         gaussian_filter1d(dn_prec_m,sigma),\n",
    "         color='blue', linestyle='dotted', alpha=alpha, label='intermediate (ssp370)')\n",
    "ax[3].plot(np.arange(dn_temp.shape[0], dn_temp.shape[0]+dn_temp_p.shape[0]),\n",
    "         gaussian_filter1d(dn_prec_o,sigma),\n",
    "         color='green', linestyle='dotted', alpha=alpha, label='optimistic (ssp126)')\n",
    "ax[3].set(xlabel='time (month)', ylabel='precipitation')\n",
    "ax[3].legend()\n",
    "\n",
    "fig.savefig('figs/tendency_metrics.eps', bbox_inches='tight')\n",
    "fig.savefig('figs/tendency_metrics.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting without sea data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npp_files = glob.glob('data/net_primary_production_on_land/historical/**/*.nc', recursive=True) # TODO: use all models\n",
    "npp_ds = np.array(xr.open_mfdataset(npp_files[1]).npp)[0]\n",
    "plt.imshow(npp_ds, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "def remove_water(data, npp_ds=npp_ds):\n",
    "\n",
    "    index = []\n",
    "    while not any(index):\n",
    "        index = [False if not np.isnan(value) else True for value in npp_ds.flatten()]\n",
    "\n",
    "    data_water_free = []\n",
    "    for img in data:\n",
    "        data_water_free.append(np.delete(img.flatten(), index))\n",
    "    \n",
    "    data = np.array(data_water_free)    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npp_files = glob.glob('data/net_primary_production_on_land/historical/**/*.nc', recursive=True) # TODO: use all models\n",
    "npp_ds = np.array(xr.open_mfdataset(np.random.choice(np.array(npp_files))).npp)\n",
    "npp_ds = np.nan_to_num(npp_ds)\n",
    "lai_dx = xr.open_mfdataset('data/leaf_area_index/historical/{}/*.nc'.format(dmodel))  # not numpy as time index needed\n",
    "lai_dx['lai'] = lai_dx['lai'].fillna(0)\n",
    "\n",
    "max_output = 10\n",
    "\n",
    "min_lai = np.min(np.array(lai_dx['lai']))\n",
    "max_lai = np.max(np.array(lai_dx['lai']))\n",
    "\n",
    "min_npp = np.min(npp_ds)\n",
    "max_npp = np.max(npp_ds)\n",
    "\n",
    "lai_hist = np.nan_to_num(remove_water(np.array(xr.open_mfdataset('data/leaf_area_index/historical/{}/*.nc'.format(dmodel))['lai'])))\n",
    "npp_hist = np.nan_to_num(remove_water(np.array(xr.open_mfdataset(np.random.choice(np.array(npp_files))).npp)))\n",
    "\n",
    "lai_future_o = remove_water(redo_minmax(np.load('results/pred_Dropout_historic_Pessimistic.npy')[:,:,:,0], min_lai, max_lai, max_output))\n",
    "npp_future_o = remove_water(redo_minmax(np.load('results/pred_Dropout_historic_Pessimistic.npy')[:,:,:,1], min_npp, max_npp, max_output))\n",
    "lai_future_m = remove_water(redo_minmax(np.load('results/pred_Dropout_historic_Medium.npy')[:,:,:,0], min_lai, max_lai, max_output))\n",
    "npp_future_m = remove_water(redo_minmax(np.load('results/pred_Dropout_historic_Medium.npy')[:,:,:,1], min_npp, max_npp, max_output))\n",
    "lai_future_p = remove_water(redo_minmax(np.load('results/pred_Dropout_historic_Optimistic.npy')[:,:,:,0], min_lai, max_lai, max_output))\n",
    "npp_future_p = remove_water(redo_minmax(np.load('results/pred_Dropout_historic_Optimistic.npy')[:,:,:,1], min_npp, max_npp, max_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 10\n",
    "offset_lai = 0.145\n",
    "offset_npp = 0.00000000305\n",
    "alpha = 0.8\n",
    "\n",
    "fig,ax = plt.subplots(2,1, figsize=(40,25))\n",
    "\n",
    "ax[0].plot(np.arange(lai_hist.shape[0]),\n",
    "            gaussian_filter1d(np.mean(lai_hist, axis=-1),sigma) ,\n",
    "            color='gray')\n",
    "ax[0].plot(np.arange(lai_hist.shape[0], lai_hist.shape[0]+lai_future_p.shape[0]),\n",
    "            gaussian_filter1d(np.mean(lai_future_p, axis=-1),sigma)  + offset_lai,\n",
    "            color='red', linestyle='dotted', alpha=alpha, label='pessimistic (ssp585)')\n",
    "ax[0].plot(np.arange(lai_hist.shape[0], lai_hist.shape[0]+lai_future_m.shape[0]),\n",
    "            gaussian_filter1d(np.mean(lai_future_m, axis=-1),sigma)  + offset_lai,\n",
    "            color='blue', linestyle='dotted', alpha=alpha, label='intermediate (ssp370)')\n",
    "ax[0].plot(np.arange(lai_hist.shape[0], lai_hist.shape[0]+lai_future_o.shape[0]),\n",
    "            gaussian_filter1d(np.mean(lai_future_o, axis=-1),sigma)  + offset_lai,\n",
    "            color='green', linestyle='dotted', alpha=alpha, label='optimistic (ssp126)')\n",
    "ax[0].set(ylabel='LAI')\n",
    "\n",
    "\n",
    "\n",
    "ax[1].plot(np.arange(npp_hist.shape[0]),\n",
    "            gaussian_filter1d(np.mean(npp_hist, axis=-1),sigma),\n",
    "            color='gray')\n",
    "\n",
    "ax[1].plot(np.arange(npp_hist.shape[0], npp_hist.shape[0]+npp_future_p.shape[0]),\n",
    "            gaussian_filter1d(np.mean(npp_future_p, axis=-1),sigma) + offset_npp,\n",
    "            color='red', linestyle='dotted', alpha=alpha, label='pessimistic (ssp585)')\n",
    "\n",
    "ax[1].plot(np.arange(npp_hist.shape[0], npp_hist.shape[0]+npp_future_m.shape[0]),\n",
    "            gaussian_filter1d(np.mean(npp_future_m, axis=-1),sigma) + offset_npp,\n",
    "            color='blue', linestyle='dotted', alpha=alpha, label='intermediate (ssp370)')\n",
    "\n",
    "ax[1].plot(np.arange(npp_hist.shape[0], npp_hist.shape[0]+npp_future_o.shape[0]),\n",
    "            gaussian_filter1d(np.mean(npp_future_o, axis=-1),sigma) + offset_npp,\n",
    "            color='green', linestyle='dotted', alpha=alpha, label='optimistic (ssp126)')\n",
    "\n",
    "ax[1].set( ylabel='NPP')\n",
    "\n",
    "fig.savefig('figs/tendency_metrics_water_free.eps', bbox_inches='tight')\n",
    "fig.savefig('figs/tendency_metrics_water_free.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
