{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data param\n",
    "day_len = 30\n",
    "batch_size = 16\n",
    "\n",
    "# model param\n",
    "num_filters = 16\n",
    "\n",
    "# training param\n",
    "epochs = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "data_models = ['GFDL-ESM4','IPSL-CM6A-LR','MPI-ESM1-2-HR']  # models for temp, prec, LAI\n",
    "dmodel = 'IPSL-CM6A-LR' # np.random.choice(np.array(data_models))  # TODO: choose a model\n",
    "\n",
    "temp_ds = np.array(xr.open_mfdataset('data/near_surface_air_temperature/historical/{}/*.nc'.format(dmodel)).tas)\n",
    "prec_ds = np.array(xr.open_mfdataset('data/precipitation_flux/historical/{}/*.nc'.format(dmodel)).pr)\n",
    "\n",
    "npp_files = glob.glob('data/net_primary_production_on_land/historical/**/*.nc', recursive=True) # TODO: use all models\n",
    "npp_ds = np.array(xr.open_mfdataset(np.random.choice(np.array(npp_files))).npp)\n",
    "npp_ds = np.nan_to_num(npp_ds)\n",
    "lai_dx = xr.open_mfdataset('data/leaf_area_index/historical/{}/*.nc'.format(dmodel))  # not numpy as time index needed\n",
    "lai_dx['lai'] = lai_dx.lai.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the outputs (LAI and NPP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max scale outputs to [0,10]\n",
    "max_output = 10\n",
    "npp_ds_s = max_output* (npp_ds-np.min(npp_ds))/(np.max(npp_ds)-np.min(npp_ds))\n",
    "lai_ds_o = lai_dx['lai'] \n",
    "lai_dx[\"lai\"] = max_output * (lai_dx.lai - np.min(lai_dx.lai))/(np.max(lai_dx.lai)-np.min(lai_dx.lai))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_max_month = 1978-(12*14)\n",
    "train_min_month = day_len//28\n",
    "\n",
    "val_max_month = 1978\n",
    "val_min_month = (day_len//28) + (1978-(12*14))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# min-max scale inputs normalized only on train data (statistics will also be used to normalize test set)\n",
    "\n",
    "train_max_index = temp_ds.shape[0] - (365*14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_temp = np.min(temp_ds)#[:train_max_index])\n",
    "max_temp = np.max(temp_ds)#[:train_max_index])\n",
    "\n",
    "min_prec = np.min(prec_ds)#[:train_max_index])\n",
    "max_prec = np.max(prec_ds)#[:train_max_index])\n",
    "\n",
    "min_input= -1\n",
    "max_input = 1\n",
    "\n",
    "temp_ds_s = (temp_ds - min_temp) * (max_input - min_input) / (max_temp - min_temp)\n",
    "                                  \n",
    "prec_ds_s = (prec_ds - min_prec) * (max_input - min_input) / (max_prec - min_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of output month indicies needed for plotting and generator\n",
    "val_months = 12*14 # 14 years of evaluation data\n",
    "\n",
    "train_max_month = 1978-val_months\n",
    "min_month = day_len//28\n",
    "\n",
    "val_max_month = 1978\n",
    "val_min_month = train_max_month +1\n",
    "\n",
    "def train_gen_data_card():\n",
    "    while True:\n",
    "        # array to append to\n",
    "        endstamp = []\n",
    "        output_day_i = np.zeros(batch_size)\n",
    "        \n",
    "        lai = np.zeros((batch_size, npp_ds.shape[1], npp_ds.shape[2]))  # batch, lon, lat\n",
    "        npp = np.zeros((batch_size, npp_ds.shape[1], npp_ds.shape[2]))\n",
    "        temp = np.zeros((batch_size, day_len, npp_ds.shape[1], npp_ds.shape[2]))  # batch, time, lon, lat\n",
    "        prec = np.zeros((batch_size, day_len, npp_ds.shape[1], npp_ds.shape[2]))\n",
    "        \n",
    "        # index of output in month\n",
    "        output_month_i = np.random.randint(train_min_month, train_max_month, size=batch_size)  # y_pred timepoint in int\n",
    "\n",
    "        # convert output index to timestamp\n",
    "        try:\n",
    "            for i in range(batch_size):\n",
    "                endstamp.append(lai_dx.indexes['time'].to_datetimeindex()[output_month_i[i]])  # cfttimeindex to datetime               \n",
    "        except:\n",
    "            for i in range(batch_size):\n",
    "                endstamp.append(lai_dx.indexes['time'][output_month_i[i]])\n",
    "\n",
    "        # convert output month index to day index\n",
    "        for i in range(batch_size):\n",
    "            output_day_i[i] = (endstamp[i] - pd.Timestamp('1850-01-01T12')).days  # output is i-th day in int\n",
    "        output_day_i = np.int_(output_day_i)\n",
    "\n",
    "        # save month-based time slices\n",
    "        lainp = np.array(lai_dx.lai)\n",
    "        for i in range(batch_size):\n",
    "            lai[i] = lainp[output_month_i[i]]\n",
    "        for i in range(batch_size):\n",
    "            npp[i] = npp_ds_s[output_month_i[i]]\n",
    "\n",
    "        # day-based metrics\n",
    "        for i in range(batch_size):\n",
    "            temp[i] = temp_ds_s[output_day_i[i]-day_len:output_day_i[i]]\n",
    "        for i in range(batch_size):\n",
    "            prec[i] = prec_ds_s[output_day_i[i]-day_len:output_day_i[i]]\n",
    "\n",
    "        # merge features\n",
    "        inputs = np.stack((temp,prec), axis=-1)  # two features\n",
    "        outputs = np.stack((lai,npp), axis=-1)\n",
    "\n",
    "        yield (inputs, outputs)\n",
    "        \n",
    "def val_gen_data_card():\n",
    "    while True:\n",
    "        # array to append to\n",
    "        endstamp = []\n",
    "        output_day_i = np.zeros(batch_size)\n",
    "        \n",
    "        lai = np.zeros((batch_size, npp_ds.shape[1], npp_ds.shape[2]))  # batch, lon, lat\n",
    "        npp = np.zeros((batch_size, npp_ds.shape[1], npp_ds.shape[2]))\n",
    "        temp = np.zeros((batch_size, day_len, npp_ds.shape[1], npp_ds.shape[2]))  # batch, time, lon, lat\n",
    "        prec = np.zeros((batch_size, day_len, npp_ds.shape[1], npp_ds.shape[2]))\n",
    "        \n",
    "        # index of output in month\n",
    "        output_month_i = np.random.randint(val_min_month, val_max_month, size=batch_size)  # y_pred timepoint in int\n",
    "\n",
    "        # convert output index to timestamp\n",
    "        try:\n",
    "            for i in range(batch_size):\n",
    "                endstamp.append(lai_dx.indexes['time'].to_datetimeindex()[output_month_i[i]])  # cfttimeindex to datetime               \n",
    "        except:\n",
    "            for i in range(batch_size):\n",
    "                endstamp.append(lai_dx.indexes['time'][output_month_i[i]])\n",
    "\n",
    "        # convert output month index to day index\n",
    "        for i in range(batch_size):\n",
    "            output_day_i[i] = (endstamp[i] - pd.Timestamp('1850-01-01T12')).days  # output is i-th day in int\n",
    "        output_day_i = np.int_(output_day_i)\n",
    "\n",
    "        # save month-based time slices\n",
    "        lainp = np.array(lai_dx.lai)\n",
    "        for i in range(batch_size):\n",
    "            lai[i] = lainp[output_month_i[i]]\n",
    "        for i in range(batch_size):\n",
    "            npp[i] = npp_ds_s[output_month_i[i]]\n",
    "\n",
    "        # day-based metrics\n",
    "        for i in range(batch_size):\n",
    "            temp[i] = temp_ds_s[output_day_i[i]-day_len:output_day_i[i]]\n",
    "        for i in range(batch_size):\n",
    "            prec[i] = prec_ds_s[output_day_i[i]-day_len:output_day_i[i]]\n",
    "\n",
    "        # merge features\n",
    "        inputs = np.stack((temp,prec), axis=-1)  # two features\n",
    "        outputs = np.stack((lai,npp), axis=-1)\n",
    "\n",
    "        yield (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf_2/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_2/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m   raise TypeError(\"Could not build a TypeSpec for %r with type %s\" %\n\u001b[0m\u001b[1;32m    466\u001b[0m                   (element, type(element).__name__))\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a TypeSpec for 0 with type int",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9de9d050bb4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen_data_card\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_gen_data_card\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_2/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_generator\u001b[0;34m(generator, output_types, output_shapes, args)\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;31m# `get_iterator_id_map_fn`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;31m# A dataset that contains all of the elements generated by a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_2/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    560\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \"\"\"\n\u001b[0;32m--> 562\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_2/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   2837\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2838\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2839\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2840\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2841\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_2/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# the value. As a fallback try converting the value to a tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         normalized_components.append(\n\u001b[0;32m---> 98\u001b[0;31m             ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m     99\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_2/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_2/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[0;32m--> 261\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    262\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_2/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_2/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_2/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m           pywrap_tfe.TFE_ContextOptionsSetLazyRemoteInputsCopy(\n\u001b[1;32m    514\u001b[0m               opts, self._lazy_remote_inputs_copy)\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mcontext_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_NewContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_DeleteContextOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_generator(train_gen_data_card, output_types = (tf.float32,tf.float32))\n",
    "train_ds = train_ds.take(30).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(val_gen_data_card, output_types = (tf.float32,tf.float32))\n",
    "val_ds = val_ds.take(30).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Convolutional LSTM-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        do = 0.2\n",
    "        self.convlstm2D_1 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
    "                                                     padding=\"same\", return_sequences=True,\n",
    "                                                      activation = \"tanh\")\n",
    "        self.bn_1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.convlstm2D_2 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
    "                                                     padding=\"same\", return_sequences=True,\n",
    "                                                      activation = \"tanh\")\n",
    "        self.bn_2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "\n",
    "        self.convlstm2D_3 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
    "                                                     padding=\"same\", return_sequences=True,\n",
    "                                                      activation = \"tanh\")\n",
    "        self.bn_3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.convlstm2D_4 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
    "                                                     padding=\"same\", return_sequences=True,\n",
    "                                                      activation = \"tanh\")\n",
    "        self.bn_4 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        # convolve over time, lat, lon. This means that we assume timesteps close to each other share local similarities\n",
    "        self.conv3d = tf.keras.layers.Conv3D(filters = 2, kernel_size = (3,3,3), \n",
    "                                             activation= \"tanh\", padding=\"same\")\n",
    "        \n",
    "        self.do3d = tf.keras.layers.Dropout(do)\n",
    "        # computed convolved sum over all time dimension to get a single time slice\n",
    "        self.bottleneck = tf.keras.layers.Conv3D(filters=1, kernel_size=1, activation=\"relu\",strides=1)\n",
    "\n",
    "\n",
    "    def call(self, x, training, input_shape):\n",
    "        # (batch, time, lat, lon, channel)\n",
    "        x = tf.ensure_shape(x, input_shape) \n",
    "        # (batch, time, lat, lon, channel)\n",
    "        x = self.convlstm2D_1(x,training= training)\n",
    "        # (batch, time, lat1, lon1, filter1)\n",
    "        x = self.bn_1(x,training = training)\n",
    "        \n",
    "        x = self.convlstm2D_2(x,training = training)\n",
    "        x = self.bn_2(x,training = training)\n",
    "        \n",
    "        x = self.convlstm2D_3(x,training = training)\n",
    "        x = self.bn_3(x,training = training)\n",
    "        \n",
    "        x = self.convlstm2D_4(x,training = training)\n",
    "        x = self.bn_4(x, training = training)\n",
    "        # (batch, time, lat4, lon4, filter4)\n",
    "        x = self.do3d(x,training= training)\n",
    "        x = self.conv3d(x)\n",
    "        # (batch, newtime, newlat, newlon, newfilter=2)\n",
    "        \n",
    "        x = tf.transpose(x, [0,4,2,3,1])\n",
    "        # (batch, 2, lat, lon, time)\n",
    "        x = self.bottleneck(x)\n",
    "        # (batch, 2, lat, lon, 1)\n",
    "        \n",
    "        x = tf.transpose(x, [0,4,2,3,1])\n",
    "        # (batch, 1, lat, lon, 2)\n",
    "        x = tf.squeeze(x,axis=1)\n",
    "        # (batch, lat, lon, 2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a custom train and evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, data, loss_function, optimizer, train_loss_metric, input_shape=input_shape):\n",
    "    '''\n",
    "    Training for one epoch.\n",
    "    '''\n",
    "    for img, target in train_ds:\n",
    "        # forward pass with GradientTape\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = model(img, training=True, input_shape=input_shape)\n",
    "            loss = loss_function(target, prediction) + tf.reduce_sum(model.losses)\n",
    "\n",
    "        # backward pass via GradienTape (auto-gradient calc)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        train_loss_metric.update_state(loss)\n",
    "        \n",
    "#@tf.function        \n",
    "def eval_step(model, ds, loss_function, loss_metric, input_shape=input_shape):\n",
    "    '''\n",
    "    Evaluation Loop.\n",
    "    '''\n",
    "    for sequence, target in ds:\n",
    "        # forward pass\n",
    "        prediction = model(sequence, training=False, input_shape=input_shape)\n",
    "        # update metrics\n",
    "        loss = loss_function(target, prediction)\n",
    "        loss_metric.update_state(loss)\n",
    "        \n",
    "    fig, axe = plt.subplots(2,3, figsize=(25,10))\n",
    "    plt.tight_layout(pad= 0.05)\n",
    "    axe[0,0].imshow(target[0,:,:,0], cmap='gray', origin='lower')\n",
    "    axe[0,1].imshow(prediction[0,:,:,0], cmap='gray', origin='lower')\n",
    "    axe[1,0].imshow(target[0,:,:,1], cmap='gray', origin='lower')\n",
    "    axe[1,1].imshow(prediction[0,:,:,1], cmap='gray', origin='lower')\n",
    "    axe[0,2].imshow(np.abs(target[0,:,:,0]-prediction[0,:,:,0]), cmap = \"bwr\", \n",
    "                    origin= \"lower\", vmin = 0,vmax =max_output)\n",
    "    axe[1,2].imshow(np.abs(target[0,:,:,1]-prediction[0,:,:,1]), cmap = \"bwr\", \n",
    "                    origin= \"lower\", vmin = 0,vmax =max_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model, Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvLSTM(num_filters=num_filters)\n",
    "input_shape = (batch_size, day_len, 36, 72, 2)\n",
    "\n",
    "# define loss\n",
    "loss_function = tf.keras.losses.MSE\n",
    "# define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a timer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    \"\"\"\n",
    "    A small class to measure time during training.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start a new timer\n",
    "        \"\"\"\n",
    "        self._start_time = time.perf_counter()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"\n",
    "        Stop the timer, and report the elapsed time\n",
    "        \"\"\"\n",
    "        if self._start_time is None:\n",
    "            print(f\"Timer is not running. Use .start() to start it\")\n",
    "            return 0\n",
    "    \n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        self._start_time = None\n",
    "        \n",
    "        return elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiating Tensorboard metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = Timer()\n",
    "\n",
    "# 2nd-order metric to take mean over all samples\n",
    "train_loss_metric = tf.keras.metrics.Mean('train_loss')\n",
    "val_loss_metric = tf.keras.metrics.Mean('val_loss')\n",
    "\n",
    "# initialize the logger for Tensorboard visualization\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train_ConvLSTM'    \n",
    "val_log_dir = 'logs/gradient_tape/' + current_time + '/val_ConvLSTM'       \n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)  \n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "times = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterating over the epochs (finally training the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f'\\n[EPOCH] ____________________{epoch}____________________')\n",
    "    \n",
    "    # training step with metrics update--------------------------------------------------------\n",
    "    timer.start()\n",
    "    \n",
    "    train_step(model, train_ds, loss_function, optimizer, train_loss_metric)\n",
    "    train_loss = train_loss_metric.result()\n",
    "    \n",
    "    with train_summary_writer.as_default():     # logging our metrics to a file which is used by tensorboard\n",
    "        tf.summary.scalar('loss', train_loss, step=epoch)\n",
    "    elapsed_time = timer.stop()\n",
    "    print(f'[{epoch}] - Finished Epoch in {elapsed_time:0.2f} seconds - train_loss: {train_loss:0.4f}')\n",
    "    \n",
    "    # evaluation step with metrics update--------------------------------------------------------\n",
    "    timer.start()\n",
    "    eval_step(model, val_ds, loss_function, loss_metric=val_loss_metric)\n",
    "    plt.show()\n",
    "    \n",
    "    val_loss = val_loss_metric.result()    \n",
    "    with val_summary_writer.as_default():       # logging our metrics to a file which is used by tensorboard\n",
    "        tf.summary.scalar('loss', val_loss, step=epoch)    \n",
    "    print(f'\\n[{epoch}] - Finished evaluation - val_loss: {val_loss:0.4f}')\n",
    "    \n",
    "    # Resetting train and validation metrics-----------------------------------------------------\n",
    "    train_loss_metric.reset_states()\n",
    "    val_loss_metric.reset_states()\n",
    "    elapsed_time = timer.stop()\n",
    "    times.append(elapsed_time)\n",
    "  \n",
    "    if epoch%3 == 0:\n",
    "        print(f'\\n[INFO] - Total time elapsed: {np.sum(times)/60:0.4f} min. Total time remaining: {(np.sum(times)/(epoch+1))*(epochs-epoch-1)/60:0.4f} min.')\n",
    "\n",
    "print(f'[INFO] - Total run time: {np.sum(times)/60:0.4f} min.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_name = \"Train_run_7\"\n",
    "\n",
    "model.save_weights(\"model_weights/\"+ training_name + \".hdf5\",save_format=\"hdf5\",overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the future of plants with our trained model\n",
    "\n",
    "It is to note that we trained the model on the full dataset again after evaluation (including the most recent 14 years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_models = ['GFDL-ESM4','IPSL-CM6A-LR','MPI-ESM1-2-HR']  # models for temp, prec, LAI\n",
    "dmodel = 'IPSL-CM6A-LR' # np.random.choice(np.array(data_models))  \n",
    "scenarios = ['ssp126', 'ssp370', 'ssp585']\n",
    "\n",
    "scenario = scenarios[0] # 0 optimistic, 1 intermediate, 2 pessimistic\n",
    "\n",
    "# load historical data\n",
    "dx_temp = xr.open_mfdataset('data/near_surface_air_temperature/historical/{}/*.nc'.format(dmodel)).tas\n",
    "dx_prec = xr.open_mfdataset('data/precipitation_flux/historical/{}/*.nc'.format(dmodel)).pr\n",
    "\n",
    "# load prediction of climate until 2100 and concatanate with historical data\n",
    "dx_temp_future = xr.open_mfdataset('data/near_surface_air_temperature/{}/{}/*.nc'.format(scenario, dmodel)).tas\n",
    "dx_temp_future = xr.concat((dx_temp, dx_temp_future), dim='time')\n",
    "dx_prec_future = xr.open_mfdataset('data/precipitation_flux/{}/{}/*.nc'.format(scenario, dmodel)).pr\n",
    "dx_prec_future = xr.concat((dx_prec, dx_prec_future), dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the future's input data with past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "train_max_index = dx_temp.shape[0] - (365*14)\n",
    "\n",
    "min_temp = np.min(dx_temp)\n",
    "max_temp = np.max(dx_temp)\n",
    "\n",
    "min_prec = np.min(dx_prec)\n",
    "max_prec = np.max(dx_prec)\n",
    "\n",
    "min_input = -1\n",
    "max_input = 1\n",
    "\n",
    "dx_temp_future = (dx_temp_future - min_temp) * (max_input - min_input) / (max_temp - min_temp)\n",
    "dx_prec_future = (dx_prec_future - min_prec) * (max_input - min_input) / (max_prec - min_prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Dataset input pipeline for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_future_climate():\n",
    "#     first_month = (2015-1850)*12 + 1  # int index of which month january 2015 is with 0 being january 1850\n",
    "\n",
    "    counter = 0\n",
    "    while counter < total_months: # predict from 2015 to 2100, excluding last month\n",
    "        cyear = (counter//12) + 2015\n",
    "        \n",
    "        cmonth = (counter+1) % 12\n",
    "        \n",
    "        if cmonth == 0:\n",
    "            cmonth = 12\n",
    "        \n",
    "        current_timestamp = pd.Timestamp(cyear, cmonth, 1)\n",
    "        input_start_timestamp = current_timestamp - pd.Timedelta(day_len-1, unit='day')\n",
    "\n",
    "        counter += 1\n",
    "       \n",
    "        data = np.stack((np.array(dx_temp_future.loc[input_start_timestamp:current_timestamp+pd.Timedelta(1,unit='day')]),\n",
    "                         np.array(dx_prec_future.loc[input_start_timestamp:current_timestamp+pd.Timedelta(1,unit='day')])),\n",
    "                         axis=-1)\n",
    "        \n",
    "        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pipeline\n",
    "ds_future = tf.data.Dataset.from_generator(generator=gen_future_climate, \n",
    "                                           output_types=(tf.float32)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvLSTM(num_filters=num_filters)\n",
    "input_shape = (batch_size, day_len, 36, 72, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load weights of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, weight_file, dataset, input_shape):\n",
    "    \"\"\"\n",
    "    Builds the model by using the call method on input and then loads the weights.\n",
    "    \"\"\"\n",
    "    for data in dataset.take(1):\n",
    "        model(data, training=False, input_shape=input_shape)\n",
    "    \n",
    "    model.load_weights(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights(model, f'model_weights/{weight_file}.hdf5', ds_future, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting 2015-2100 under the previously specified scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for data in ds_future:\n",
    "    \n",
    "    try:\n",
    "        predictions = np.concatenate((predictions, model(data, training=False, input_shape=input_shape)), axis=0)    \n",
    "    except Exception as e:\n",
    "        predictions = model(data, training=False, input_shape=input_shape)\n",
    "    \n",
    "    print(f'[INFO] - {counter/int(total_months/batch_size)*100:.2f}% finished')\n",
    "    counter += 1\n",
    "\n",
    "print('[INFO] - Done')\n",
    "\n",
    "np.save('results/pred_{}'.format(weight_file), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the results in an animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.load('results/pred_{}.npy'.format(\"optimistic\"))\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"  # allow animation for jupyter\n",
    "plt.rcParams['xtick.bottom'] = False\n",
    "plt.rcParams['xtick.labelbottom'] = False\n",
    "plt.rcParams['ytick.left'] = False\n",
    "plt.rcParams['ytick.labelleft'] = False\n",
    "\n",
    "frames_lai = []  # append each image\n",
    "frames_npp = []\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)\n",
    "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "ax.set_axis_off()\n",
    "fig.add_axes(ax)\n",
    "\n",
    "for timeindex in range(total_months):  # animate for 1 yr\n",
    "\n",
    "    frames_lai.append([plt.imshow(predictions[timeindex,:,:,0], # LAI\n",
    "                              cmap='gray', origin='lower', animated=True)])\n",
    "    frames_npp.append([plt.imshow(predictions[timeindex,:,:,1], # NPP\n",
    "                              cmap='gray', origin='lower', animated=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani_lai = matplotlib.animation.ArtistAnimation(fig, frames_lai, interval=100, blit=True, repeat=True)\n",
    "# ani.save('figs/pred_v1_{}_LAI.gif'.format(scenario), writer='imagemagick', fps=60)\n",
    "ani_lai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani_npp = matplotlib.animation.ArtistAnimation(fig, frames_npp, interval=100, blit=True, repeat=True)\n",
    "# ani.save('figs/pred_v1_{}_LAI.gif'.format(scenario), writer='imagemagick', fps=60)\n",
    "ani_npp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
