{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.convlstm2D_1 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
    "                                                     padding=\"same\",return_sequences=True)\n",
    "        \n",
    "        \n",
    "        self.bn_1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.convlstm2D_2 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
    "                                                     padding=\"same\",return_sequences=True)\n",
    "\n",
    "        self.bn_2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "\n",
    "        self.convlstm2D_3 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
    "                                                     padding=\"same\",return_sequences=True)\n",
    "\n",
    "        self.bn_3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.convlstm2D_4 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
    "                                                     padding=\"same\",return_sequences=True)\n",
    "\n",
    "        self.bn_4 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv3d = tf.keras.layers.Conv3D(filters = 2, kernel_size = (3,3,3), \n",
    "                                             activation= \"relu\", padding=\"same\")\n",
    "\n",
    "    def call(self,x,training):\n",
    "        \n",
    "        x = self.convlstm2D_1(x)\n",
    "        x = self.bn_1(x,training)\n",
    "        x = self.convlstm2D_2(x)\n",
    "        x = self.bn_2(x,training)\n",
    "        x = self.convlstm2D_3(x)\n",
    "        x = self.bn_3(x,training)\n",
    "        x = self.convlstm2D_4(x)\n",
    "        x = self.bn_4(x, training)\n",
    "        x = self.conv3d(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, data, loss_function, optimizer, train_loss_metric, train_acc_metric):\n",
    "    '''\n",
    "    Training for one epoch.\n",
    "    '''\n",
    "    for img, target in train_ds:\n",
    "        # forward pass with GradientTape\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = model(img, training=True)\n",
    "            tf.keras.layers.Flatten()\n",
    "            loss = loss_function(target, prediction) + tf.reduce_sum(model.losses)\n",
    "\n",
    "        # backward pass via GradienTape (auto-gradient calc)\n",
    "        gradients = tape.gradient(loss_reg, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        train_loss_metric.update_state(loss)\n",
    "        train_acc_metric.update_state(target, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv_lstm_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_4 (ConvLSTM2D)  multiple                  60640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch multiple                  160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_5 (ConvLSTM2D)  multiple                  115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch multiple                  160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_6 (ConvLSTM2D)  multiple                  115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch multiple                  160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_7 (ConvLSTM2D)  multiple                  115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch multiple                  160       \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            multiple                  2162      \n",
      "=================================================================\n",
      "Total params: 409,522\n",
      "Trainable params: 409,202\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CONV_FILTERS = 40\n",
    "\n",
    "#Shape: None(unspecified) batches, 120 timesteps(days), 72 (latitudes), 36 (longitudes), 2(temperature&precipitation)\n",
    "input_shape = (16,120,72,36,2) \n",
    "\n",
    "model = ConvLSTM(num_filters = 40)\n",
    "\n",
    "model.build((16,120,72,36,2))\n",
    "model.summary() # shows number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b9ebdcb8f0a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0003\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "learning_rate = 0.0003\n",
    "\n",
    "loss_function = tf.keras.losses.MSE\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy('train_accuracy')\n",
    "test_acc_metric = tf.keras.metrics.CategoricalAccuracy('test_accuracy')\n",
    "\n",
    "train_loss_metric = tf.keras.metrics.Mean('train_loss')\n",
    "test_loss_metric = tf.keras.metrics.Mean('test_loss')\n",
    "\n",
    "# initialize the logger for Tensorboard visualization\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train_ResNet'      # defining the log dir\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test_ResNet'        # defining the log dir\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)  # training logger\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)    # test logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train_loss: {train_loss:0.4f}, train_acc: {train_acc:0.4f}, test_loss: {test_loss:0.4f}, test_acc: {test_acc:0.4f}')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n[EPOCH] ____________________{epoch}____________________')\n",
    "    \n",
    "    # training step with metrics update--------------------------------------------------------\n",
    "    timer.start()\n",
    "\n",
    "    train_step(model, train_ds, loss_function, optimizer, train_loss_metric, train_acc_metric)\n",
    "\n",
    "    # Evaluating training metrics\n",
    "    train_loss = train_loss_metric.result()\n",
    "    train_acc = train_acc_metric.result()\n",
    "    \n",
    "    with train_summary_writer.as_default():     # logging our metrics to a file which is used by tensorboard\n",
    "        tf.summary.scalar('loss', train_loss, step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_acc, step=epoch)\n",
    "\n",
    "    \n",
    "    elapsed_time = timer.stop()\n",
    "    \n",
    "    print(f'[{epoch}] - Finished Epoch in {elapsed_time:0.2f} seconds - train_loss: {train_loss:0.4f}, train_acc: {train_acc:0.4f}')\n",
    "    \n",
    "    # evaluation step with metrics update--------------------------------------------------------\n",
    "    timer.start()\n",
    "\n",
    "    eval_step(model, val_ds, loss_function, \n",
    "              loss_metric=val_loss_metric, \n",
    "              acc_metric=val_acc_metric)\n",
    "\n",
    "    # Evaluating validation metrics\n",
    "    val_loss = val_loss_metric.result()\n",
    "    val_acc = val_acc_metric.result()\n",
    "    \n",
    "    with test_summary_writer.as_default():       # logging our metrics to a file which is used by tensorboard\n",
    "        tf.summary.scalar('loss', val_loss, step=epoch)\n",
    "        tf.summary.scalar('accuracy', val_acc, step=epoch)\n",
    "    \n",
    "    print(f'\\n[{epoch}] - Finished evaluation - val_loss: {val_loss:0.4f}, test_accuracy: {test_acc:0.4f}')\n",
    "    \n",
    "    # Resetting train and validation metrics-----------------------------------------------------\n",
    "    train_acc_metric.reset_states()\n",
    "    val_acc_metric.reset_states()\n",
    "    train_loss_metric.reset_states()\n",
    "    val_loss_metric.reset_states()\n",
    "    \n",
    "    elapsed_time = timer.stop()\n",
    "    times.append(elapsed_time)\n",
    "  \n",
    "    if epoch%3 == 0:\n",
    "        print(f'\\n[INFO] - Total time elapsed: {np.sum(times)/60:0.4f} min. Total time remaining: {(np.sum(times)/(epoch+1))*(epochs-epoch-1)/60:0.4f} min.')\n",
    "\n",
    "print(f'[INFO] - Total run time: {np.sum(times)/60:0.4f} min.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
