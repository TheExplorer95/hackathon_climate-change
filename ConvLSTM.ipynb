{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "time_len = 10  # how long each training sample should be, in months\n",
    "models = ['GFDL-ESM4','IPSL-CM6A-LR','MPI-ESM1-2-HR']  # models for temp, prec, LAI\n",
    "\n",
    "def gen_data_card():  \n",
    "    model = np.random.choice(np.array(models))  # which of 3 models to choose from\n",
    "    \n",
    "    # MONTHLY PICK\n",
    "    start_year = np.random.randint(1850,2014+1) # randomly select a start year of a time slice\n",
    "    start_month = np.random.randint(1,12+1)\n",
    "    \n",
    "    end_year = start_year + ((start_month+time_len-1) // 12)\n",
    "    end_month = (start_month+time_len) % 12\n",
    "    \n",
    "    if end_month == 0:\n",
    "        end_month = 12\n",
    "        \n",
    "    month_index_start = (start_year-1850)*12 + start_month  # convert date into index with 01-1850 as 0\n",
    "    month_index_end = month_index_start + time_len\n",
    "    \n",
    "    # select appropriate time slices\n",
    "    temp = xr.open_mfdataset('near_surface_air_temperature/historical/{}/*.nc'.format(model))\n",
    "    temp = temp.tas.loc[\"{}-{}-16\".format(start_year, start_month):\"{}-{}-16\".format(end_year, end_month)]  \n",
    "    \n",
    "    prec = xr.open_mfdataset('precipitation_flux/historical/{}/*.nc'.format(model))\n",
    "    prec = prec.pr.loc[\"{}-{}-16\".format(start_year, start_month):\"{}-{}-16\".format(end_year, end_month)]  \n",
    "    \n",
    "    lai = xr.open_mfdataset('leaf_area_index/historical/{}/*.nc'.format(model))\n",
    "    lai = np.array(lai.lai)[month_index_start:month_index_end]\n",
    "          \n",
    "    # TODO: currently select randomly, but averaging or using only one is also an option\n",
    "    npp_files = glob.glob('net_primary_production_on_land/historical/**/*.nc', recursive=True) \n",
    "    npp = xr.open_mfdataset(np.random.choice(np.array(npp_files)))\n",
    "    npp = np.array(npp.npp)[month_index_start:month_index_end]\n",
    "                \n",
    "    # concatanate data\n",
    "    inputs = np.array(xr.concat((temp,prec), dim='lat'))  # two maps next to each other\n",
    "    outputs = np.concatenate((lai,npp), axis=1)\n",
    "    \n",
    "    yield(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_generator(gen_data_card,output_types = (tf.float32,tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 1853 1863\n",
      "MPI-ESM1-2-HR 2004 5 2005 3\n",
      "(<tf.Tensor: shape=(305, 72, 72), dtype=float32, numpy=\n",
      "array([[[2.29741730e+02, 2.30782852e+02, 2.32843491e+02, ...,\n",
      "         2.26571136e+02, 2.27270660e+02, 2.28691040e+02],\n",
      "        [2.51899918e+02, 2.49397278e+02, 2.46977905e+02, ...,\n",
      "         2.52794281e+02, 2.55649048e+02, 2.54411560e+02],\n",
      "        [2.47973114e+02, 2.45883698e+02, 2.44359314e+02, ...,\n",
      "         2.51481918e+02, 2.47000549e+02, 2.47164154e+02],\n",
      "        ...,\n",
      "        [0.00000000e+00, 2.43030081e-06, 2.77330400e-06, ...,\n",
      "         1.46517868e-06, 1.73286833e-06, 1.56242186e-06],\n",
      "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [1.73843864e-06, 2.09214227e-06, 1.88455954e-06, ...,\n",
      "         8.90273441e-07, 0.00000000e+00, 1.59804267e-06]],\n",
      "\n",
      "       [[2.29186951e+02, 2.29513275e+02, 2.30194580e+02, ...,\n",
      "         2.26919373e+02, 2.27798615e+02, 2.28777176e+02],\n",
      "        [2.47875809e+02, 2.47096481e+02, 2.44815521e+02, ...,\n",
      "         2.48843430e+02, 2.48687531e+02, 2.48657471e+02],\n",
      "        [2.47104919e+02, 2.44587219e+02, 2.40680695e+02, ...,\n",
      "         2.54424530e+02, 2.47412811e+02, 2.46372406e+02],\n",
      "        ...,\n",
      "        [0.00000000e+00, 1.73884303e-06, 2.62916524e-06, ...,\n",
      "         1.35902735e-06, 0.00000000e+00, 0.00000000e+00],\n",
      "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "         1.96266569e-06, 1.68722659e-06, 8.44103795e-07],\n",
      "        [3.66486393e-06, 3.79125981e-06, 3.62547735e-06, ...,\n",
      "         2.74840522e-06, 3.18990169e-06, 3.95346342e-06]],\n",
      "\n",
      "       [[2.27939148e+02, 2.28403000e+02, 2.30238815e+02, ...,\n",
      "         2.23597580e+02, 2.25331482e+02, 2.26781647e+02],\n",
      "        [2.45435730e+02, 2.45548035e+02, 2.44281082e+02, ...,\n",
      "         2.48577942e+02, 2.46903061e+02, 2.45145660e+02],\n",
      "        [2.48871338e+02, 2.47028885e+02, 2.44115540e+02, ...,\n",
      "         2.48591400e+02, 2.44893097e+02, 2.47072723e+02],\n",
      "        ...,\n",
      "        [2.08140432e-06, 2.50520088e-06, 2.72471129e-06, ...,\n",
      "         2.10438839e-06, 2.06713139e-06, 2.32726848e-06],\n",
      "        [1.81998507e-06, 2.10784856e-06, 2.41143493e-06, ...,\n",
      "         2.15632076e-06, 2.16047442e-06, 2.05418519e-06],\n",
      "        [3.62612400e-06, 3.30110697e-06, 3.40314955e-06, ...,\n",
      "         3.14360022e-06, 3.50955065e-06, 3.79145558e-06]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[2.34436676e+02, 2.36025513e+02, 2.36318008e+02, ...,\n",
      "         2.33178604e+02, 2.33833832e+02, 2.34575592e+02],\n",
      "        [2.46862534e+02, 2.47911285e+02, 2.47118378e+02, ...,\n",
      "         2.49094604e+02, 2.49683990e+02, 2.48547440e+02],\n",
      "        [2.49862610e+02, 2.48770874e+02, 2.47229919e+02, ...,\n",
      "         2.54953857e+02, 2.48064560e+02, 2.47740341e+02],\n",
      "        ...,\n",
      "        [0.00000000e+00, 1.54836596e-06, 2.30099158e-06, ...,\n",
      "         1.44151709e-05, 0.00000000e+00, 0.00000000e+00],\n",
      "        [1.11985792e-05, 1.01930782e-05, 6.19968023e-06, ...,\n",
      "         6.30014529e-06, 3.72191198e-06, 1.21658395e-05],\n",
      "        [3.76471621e-06, 3.25728161e-06, 2.35329162e-06, ...,\n",
      "         8.91217678e-06, 3.40425709e-06, 3.76306275e-06]],\n",
      "\n",
      "       [[2.31431381e+02, 2.32059280e+02, 2.32242767e+02, ...,\n",
      "         2.29708694e+02, 2.30427856e+02, 2.31137344e+02],\n",
      "        [2.44009521e+02, 2.44604828e+02, 2.44302902e+02, ...,\n",
      "         2.47269791e+02, 2.47091766e+02, 2.45514053e+02],\n",
      "        [2.50909409e+02, 2.50158432e+02, 2.49518890e+02, ...,\n",
      "         2.55425110e+02, 2.49778198e+02, 2.49368347e+02],\n",
      "        ...,\n",
      "        [3.61541152e-06, 2.14333500e-06, 1.08169434e-06, ...,\n",
      "         3.61579214e-06, 0.00000000e+00, 0.00000000e+00],\n",
      "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [1.66518316e-06, 2.75965272e-06, 4.02771366e-06, ...,\n",
      "         1.09830489e-06, 0.00000000e+00, 0.00000000e+00]],\n",
      "\n",
      "       [[2.31208710e+02, 2.31859436e+02, 2.32066284e+02, ...,\n",
      "         2.28190399e+02, 2.29550293e+02, 2.30537964e+02],\n",
      "        [2.44289963e+02, 2.42385620e+02, 2.42067688e+02, ...,\n",
      "         2.47361694e+02, 2.47385162e+02, 2.44320190e+02],\n",
      "        [2.52600616e+02, 2.51796204e+02, 2.51474304e+02, ...,\n",
      "         2.55142059e+02, 2.49697083e+02, 2.50416519e+02],\n",
      "        ...,\n",
      "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "         6.76121772e-07, 1.34750951e-06, 0.00000000e+00],\n",
      "        [1.26787461e-06, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 1.79088590e-06],\n",
      "        [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32)>, <tf.Tensor: shape=(10, 72, 72), dtype=float32, numpy=\n",
      "array([[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        [nan, nan, nan, ..., nan, nan, nan]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        [nan, nan, nan, ..., nan, nan, nan]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        [nan, nan, nan, ..., nan, nan, nan]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        [nan, nan, nan, ..., nan, nan, nan]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        [nan, nan, nan, ..., nan, nan, nan]],\n",
      "\n",
      "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        [nan, nan, nan, ..., nan, nan, nan],\n",
      "        [nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "for i in train_ds.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(tf.keras.Model):\n",
    "    def __init__(self, num_filters,months):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.convlstm2D_1 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
    "                                                     padding=\"same\",return_sequences=True)\n",
    "        \n",
    "        \n",
    "        self.bn_1 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.convlstm2D_2 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
    "                                                     padding=\"same\",return_sequences=True)\n",
    "\n",
    "        self.bn_2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "\n",
    "        self.convlstm2D_3 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
    "                                                     padding=\"same\",return_sequences=True)\n",
    "\n",
    "        self.bn_3 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.convlstm2D_4 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
    "                                                     padding=\"same\",return_sequences=True)\n",
    "\n",
    "        self.bn_4 = tf.keras.layers.BatchNormalization()\n",
    "        \n",
    "        self.conv3d = tf.keras.layers.Conv3D(filters = 2, kernel_size = (3,3,3), \n",
    "                                             activation= \"relu\", padding=\"same\")\n",
    "        \n",
    "        #self.bottleneck = tf.keras.layers.Conv2D(filters = months, kernel_size=1,\n",
    "        #                                        strides = 1, activation = \"relu\",\n",
    "        #                                        padding =\"same\")\n",
    "        \n",
    "\n",
    "    def call(self,x,training):\n",
    "        \n",
    "        x = self.convlstm2D_1(x)\n",
    "        x = self.bn_1(x,training)\n",
    "        x = self.convlstm2D_2(x)\n",
    "        x = self.bn_2(x,training)\n",
    "        x = self.convlstm2D_3(x)\n",
    "        x = self.bn_3(x,training)\n",
    "        x = self.convlstm2D_4(x)\n",
    "        x = self.bn_4(x, training)\n",
    "        x = self.conv3d(x)\n",
    "        \n",
    "        # bottleneck (change time_step dim to be channel dimension so we can use the bottleneck)\n",
    "        #x = tf.transpose(x, [0,4,2,3,1])\n",
    "        #x = self.bottleneck(x)\n",
    "        # change back to desired dimensions\n",
    "        #x = tf.transpose(x, [0,4,2,3,1])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, data, loss_function, optimizer, train_loss_metric, train_acc_metric):\n",
    "    '''\n",
    "    Training for one epoch.\n",
    "    '''\n",
    "    for img, target in train_ds:\n",
    "        # forward pass with GradientTape\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = model(img, training=True)\n",
    "            tf.keras.layers.Flatten()\n",
    "            loss = loss_function(target, prediction) + tf.reduce_sum(model.losses)\n",
    "\n",
    "        # backward pass via GradienTape (auto-gradient calc)\n",
    "        gradients = tape.gradient(loss_reg, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        train_loss_metric.update_state(loss)\n",
    "        train_acc_metric.update_state(target, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv_lstm_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_lst_m2d_4 (ConvLSTM2D)  multiple                  60640     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch multiple                  160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_5 (ConvLSTM2D)  multiple                  115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch multiple                  160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_6 (ConvLSTM2D)  multiple                  115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch multiple                  160       \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_7 (ConvLSTM2D)  multiple                  115360    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch multiple                  160       \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            multiple                  2162      \n",
      "=================================================================\n",
      "Total params: 409,522\n",
      "Trainable params: 409,202\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CONV_FILTERS = 40\n",
    "months = 10\n",
    "\n",
    "#Shape: None(unspecified) batches, timesteps(in days), 72 (latitudes), 36 (longitudes), 2(temperature&precipitation)\n",
    "input_shape = (16, months*30.5, 72, 36, 2)\n",
    "\n",
    "model = ConvLSTM(num_filters = 40, months = 10)\n",
    "\n",
    "model.build((16,120,72,36,2))\n",
    "model.summary() # shows number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b9ebdcb8f0a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0003\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "learning_rate = 0.0003\n",
    "\n",
    "loss_function = tf.keras.losses.MSE\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy('train_accuracy')\n",
    "test_acc_metric = tf.keras.metrics.CategoricalAccuracy('test_accuracy')\n",
    "\n",
    "train_loss_metric = tf.keras.metrics.Mean('train_loss')\n",
    "test_loss_metric = tf.keras.metrics.Mean('test_loss')\n",
    "\n",
    "# initialize the logger for Tensorboard visualization\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train_ResNet'      # defining the log dir\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test_ResNet'        # defining the log dir\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)  # training logger\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)    # test logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train_loss: {train_loss:0.4f}, train_acc: {train_acc:0.4f}, test_loss: {test_loss:0.4f}, test_acc: {test_acc:0.4f}')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n[EPOCH] ____________________{epoch}____________________')\n",
    "    \n",
    "    # training step with metrics update--------------------------------------------------------\n",
    "    timer.start()\n",
    "\n",
    "    train_step(model, train_ds, loss_function, optimizer, train_loss_metric, train_acc_metric)\n",
    "\n",
    "    # Evaluating training metrics\n",
    "    train_loss = train_loss_metric.result()\n",
    "    train_acc = train_acc_metric.result()\n",
    "    \n",
    "    with train_summary_writer.as_default():     # logging our metrics to a file which is used by tensorboard\n",
    "        tf.summary.scalar('loss', train_loss, step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_acc, step=epoch)\n",
    "\n",
    "    \n",
    "    elapsed_time = timer.stop()\n",
    "    \n",
    "    print(f'[{epoch}] - Finished Epoch in {elapsed_time:0.2f} seconds - train_loss: {train_loss:0.4f}, train_acc: {train_acc:0.4f}')\n",
    "    \n",
    "    # evaluation step with metrics update--------------------------------------------------------\n",
    "    timer.start()\n",
    "\n",
    "    eval_step(model, val_ds, loss_function, \n",
    "              loss_metric=val_loss_metric, \n",
    "              acc_metric=val_acc_metric)\n",
    "\n",
    "    # Evaluating validation metrics\n",
    "    val_loss = val_loss_metric.result()\n",
    "    val_acc = val_acc_metric.result()\n",
    "    \n",
    "    with test_summary_writer.as_default():       # logging our metrics to a file which is used by tensorboard\n",
    "        tf.summary.scalar('loss', val_loss, step=epoch)\n",
    "        tf.summary.scalar('accuracy', val_acc, step=epoch)\n",
    "    \n",
    "    print(f'\\n[{epoch}] - Finished evaluation - val_loss: {val_loss:0.4f}, test_accuracy: {test_acc:0.4f}')\n",
    "    \n",
    "    # Resetting train and validation metrics-----------------------------------------------------\n",
    "    train_acc_metric.reset_states()\n",
    "    val_acc_metric.reset_states()\n",
    "    train_loss_metric.reset_states()\n",
    "    val_loss_metric.reset_states()\n",
    "    \n",
    "    elapsed_time = timer.stop()\n",
    "    times.append(elapsed_time)\n",
    "  \n",
    "    if epoch%3 == 0:\n",
    "        print(f'\\n[INFO] - Total time elapsed: {np.sum(times)/60:0.4f} min. Total time remaining: {(np.sum(times)/(epoch+1))*(epochs-epoch-1)/60:0.4f} min.')\n",
    "\n",
    "print(f'[INFO] - Total run time: {np.sum(times)/60:0.4f} min.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
