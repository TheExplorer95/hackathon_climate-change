{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "convertible-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    " \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-mountain",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incredible-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    \"\"\"\n",
    "    A small class to measure time during training.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start a new timer\n",
    "        \"\"\"\n",
    "        self._start_time = time.perf_counter()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"\n",
    "        Stop the timer, and report the elapsed time\n",
    "        \"\"\"\n",
    "        if self._start_time is None:\n",
    "            print(f\"Timer is not running. Use .start() to start it\")\n",
    "            return 0\n",
    "    \n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        self._start_time = None\n",
    "        \n",
    "        return elapsed_time\n",
    "    \n",
    "def eval_metrics(metrics, metric_lists):\n",
    "    for m, m_l in zip(metrics, metric_lists):\n",
    "        val = m.result()\n",
    "        m_l.append(val)\n",
    "        m.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "advised-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen():\n",
    "    \n",
    "#     time_steps_days = 170*365\n",
    "#    time_steps_months = 170*12 not necessary, as we interpolate/repeat labels to correspond to the time_steps\n",
    "    long = 36\n",
    "    lat = 72\n",
    "    \n",
    "    data = np.random.randint(low = 0, high = 100, size=(long,lat,2), dtype=np.int32)\n",
    "#     labels = np.random.randint(low=-5, high = 200,size=(lat,long,2), dtype=np.int32)\n",
    "    \n",
    "    while True:\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "respected-finish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 72, 2, 1)\n",
      "(4, 36, 72, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "input_shape =(4, 36, 72, 2, 1)\n",
    "x = tf.random.normal(input_shape)\n",
    "print(input_shape[1:])\n",
    "\n",
    "y = tf.keras.layers.Conv3D(5, 3, activation='relu', input_shape=input_shape[1:], padding='same')(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-florence",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "packed-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        \n",
    "        self.layers = []\n",
    "        \n",
    "        self.layers.append(tf.keras.layers.Conv2D(filters=32,\n",
    "                                                  kernel_size=3,\n",
    "                                                  strides=2,\n",
    "                                                  padding='same',\n",
    "                                                  input_shape=input_dim))  \n",
    "        self.layers.append(tf.keras.layers.BatchNormalization())\n",
    "        self.layers.append(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "        self.layers.append(tf.keras.layers.Conv2D(filters=64,\n",
    "                                                  kernel_size=3,\n",
    "                                                  strides=2,\n",
    "                                                  padding='same'))  \n",
    "        self.layers.append(tf.keras.layers.BatchNormalization())\n",
    "        self.layers.append(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "        self.layers.append(tf.keras.layers.Flatten())\n",
    "        self.layers.append(tf.keras.layers.Dense(latent_dim, activation='relu'))\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        for layer in self.layers:\n",
    "            try:  # training argument only for BN layer\n",
    "                x = layer(x, training) \n",
    "#                 print(x.shape)\n",
    "            except:\n",
    "                x = layer(x)\n",
    "#                 print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    def make_untrainable():\n",
    "        for layer in self.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "class CNN_Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, latent_dim, output_dim, restore_shape):\n",
    "        super(CNN_Decoder, self).__init__()\n",
    "        self.layers = []\n",
    "        \n",
    "        # dense layer to restore dim of flattend data\n",
    "        self.layers.append(tf.keras.layers.Dense(units=int(tf.math.reduce_prod((restore_shape))),\n",
    "                                                 input_shape=(latent_dim,)))\n",
    "        self.layers.append(tf.keras.layers.BatchNormalization())\n",
    "        self.layers.append(tf.keras.layers.Activation('relu'))\n",
    "        \n",
    "        self.layers.append(tf.keras.layers.Reshape(target_shape=restore_shape))        \n",
    "        self.layers.append(tf.keras.layers.Conv2DTranspose(filters=32,\n",
    "                                                           kernel_size=3,\n",
    "                                                           strides=2,\n",
    "                                                           padding='same'))\n",
    "        self.layers.append(tf.keras.layers.BatchNormalization())\n",
    "        self.layers.append(tf.keras.layers.Activation('relu'))\n",
    "       \n",
    "        self.layers.append(tf.keras.layers.Conv2DTranspose(filters=2,\n",
    "                                                  kernel_size=3,\n",
    "                                                  strides=2,\n",
    "                                                  padding='same'))  \n",
    "        self.layers.append(tf.keras.layers.BatchNormalization())\n",
    "        self.layers.append(tf.keras.layers.Activation('sigmoid'))\n",
    "       \n",
    "    def call(self, x, training=False):\n",
    "        for layer in self.layers:\n",
    "            try:  # training argument only for BN layer\n",
    "                x = layer(x, training) \n",
    "#                 print(x.shape)\n",
    "            except:\n",
    "                x = layer(x)\n",
    "#                 print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    def make_untrainable():\n",
    "        for layer in self.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "class CNN_Autoencoder(tf.keras.Model):\n",
    "    def __init__(self, input_dim, latent_dim, restore_shape):\n",
    "        super(CNN_Autoencoder, self).__init__()\n",
    "        # encoder and decoder are symmetric\n",
    "        self.encoder = CNN_Encoder(input_dim=input_dim,\n",
    "                                   latent_dim=latent_dim)\n",
    "        \n",
    "        self.decoder = CNN_Decoder(latent_dim=latent_dim,\n",
    "                                   output_dim=input_dim,\n",
    "                                   restore_shape=restore_shape)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        x = self.encoder(x, training)\n",
    "        self.latent_repr = x  # keep latent_repr as property in case it should be analyzed\n",
    "        x = self.decoder(x, training)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def make_untrainable():\n",
    "        self.trainable = False\n",
    "        self.encoder.make_untrainable()\n",
    "        self.decoder.make_untrainable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "legitimate-orchestra",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 36, 72, 2)\n",
      "(4, 18, 36, 32)\n",
      "(4, 18, 36, 32)\n",
      "(4, 18, 36, 32)\n",
      "(4, 9, 18, 64)\n",
      "(4, 9, 18, 64)\n",
      "(4, 9, 18, 64)\n",
      "(4, 10368)\n",
      "(4, 1000)\n",
      "(4, 10368)\n",
      "(4, 10368)\n",
      "(4, 10368)\n",
      "(4, 9, 18, 64)\n",
      "(4, 18, 36, 32)\n",
      "(4, 18, 36, 32)\n",
      "(4, 18, 36, 32)\n",
      "(4, 36, 72, 2)\n",
      "(4, 36, 72, 2)\n",
      "(4, 36, 72, 2)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (4, 36, 72, 2)\n",
    "data = tf.random.normal(input_shape)\n",
    "print(input_shape)\n",
    "\n",
    "model_AE = CNN_Autoencoder(input_dim=(None, lat, long, 2), \n",
    "                           latent_dim=1000,\n",
    "                           restore_shape=(9, 18, 64))\n",
    "\n",
    "latent = model_AE.encoder(data)\n",
    "x = model_AE.decoder(latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-angel",
   "metadata": {},
   "source": [
    "# Training the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_AE(model, train_ds, loss_function, optimizer, train_loss_metric):\n",
    "    '''\n",
    "    Training for one epoch. Adjusted for Autoencoder as there are no acc_metric.\n",
    "    '''\n",
    "    for img, label in train_ds:  # there are no (input,label) pairs\n",
    "        with tf.GradientTape() as tape:\n",
    "            # forward pass\n",
    "            prediction = model(img, training=True)\n",
    "            loss = loss_function(img, prediction) \n",
    "\n",
    "        # backward pass\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        train_loss_metric.update_state(loss)\n",
    "\n",
    "@tf.function\n",
    "def eval_step_AE(model, ds, loss_function, loss_metric):\n",
    "    '''\n",
    "    Evaluate without training. Adjusted for autoencoder. \n",
    "    Return a random image and reconstructed version of it.\n",
    "    '''\n",
    "    prediction = 0.0\n",
    "    img = 0.0\n",
    "\n",
    "    for img, label in ds:\n",
    "        # forward pass\n",
    "        prediction = model(img, training=False)\n",
    "        \n",
    "        # update metrics\n",
    "        loss = loss_function(img, prediction)\n",
    "        loss_metric.update_state(loss)\n",
    "        \n",
    "    return img, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "epochs = 25\n",
    "laerning_rate = 0.001\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "timer = Timer()\n",
    "\n",
    "model_AE = CNN_Autoencoder(input_dim=(None,28,28,1), \n",
    "                     latent_dim=10,\n",
    "                     restore_shape=(9, 18, 64))\n",
    "\n",
    "loss_function = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(laerning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_AE = [tf.keras.metrics.Mean('train_loss'),\n",
    "              tf.keras.metrics.Mean('test_loss')]\n",
    "\n",
    "# loss[0] - train, loss[1] - test\n",
    "losses_AE = []\n",
    "\n",
    "times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[INFO] - Evaluating the Dataset on the {model_AE.name} before training.')\n",
    "timer.start()\n",
    "\n",
    "# evaluate once before training \n",
    "eval_step_AE(model_AE,\n",
    "             train_ds,\n",
    "             loss_function, \n",
    "             loss_metric=train_loss_metric)\n",
    "\n",
    "eval_step_AE(model_AE,  \n",
    "             test_ds,\n",
    "             loss_function, \n",
    "             loss_metric=test_loss_metric)\n",
    "\n",
    "# Evaluate the metrics\n",
    "eval_metrics(metrics=metrics_AE, metric_lists=losses_AE)\n",
    "\n",
    "# Evaluate the timer\n",
    "elapsed_time = timer.stop()\n",
    "times.append(elapsed_time)\n",
    "\n",
    "print(f'train_loss: {train_loss:0.4f}, test_loss: {test_loss:0.4f}')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n[EPOCH] ____________________{epoch}____________________')\n",
    "    \n",
    "    timer.start()\n",
    "    \n",
    "    # Training step    \n",
    "    train_step_AE(model_AE, train_ds, loss_function, optimizer, train_loss_metric)\n",
    "    \n",
    "    # Test step    \n",
    "    img_original, img_reconstructed = eval_step_AE(model_AE, test_ds, loss_function, test_loss_metric)\n",
    "    \n",
    "    # Evaluate the metrics\n",
    "    eval_metrics(metrics=metrics_AE, metric_lists=losses_AE)\n",
    "    \n",
    "    elapsed_time = timer.stop()\n",
    "    times.append(elapsed_time)\n",
    "\n",
    "    print(f'[{epoch}] - Finished Epoch in {elapsed_time:0.2f} seconds - train_loss: {train_loss:0.4f}; test_loss: {test_loss:0.4f}')\n",
    "    \n",
    "    # print progress every while\n",
    "    if epoch%5 == 0:\n",
    "        print(f'\\n[INFO] - Total time elapsed: {np.sum(times)/60:0.4f} min. Total time remaining: {(np.sum(times)/(epoch+1))*(EPOCHS-epoch-1)/60:0.4f} min.')\n",
    "        \n",
    "        # Visualize reconstructed image      \n",
    "        plt.figure(figsize=(1, 2))\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.title('Original Image')\n",
    "        plt.imshow(img_original[0,:,:,0], cmap='gray')\n",
    "\n",
    "        plt.subplot(122)\n",
    "        plt.title('Reconstructed Image')\n",
    "        plt.imshow(img_reconstructed[0,:,:,0], cmap='gray')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "                    \n",
    "print(f'\\n[INFO] - Total run time: {np.sum(times)/60:0.4f} min.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-project",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "perceived-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, rnn_units, autoencoder):\n",
    "        super().__init__(GRU_AE, self)\n",
    "        self.autoencoder = autoencoder\n",
    "        self.autoencoder.make_untrainable()      \n",
    "        \n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        x = self.autoencoder.encode(x)\n",
    "        x = self.gru(x, training=training)\n",
    "        x = self.autoencoder.decode(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "epochs = 25\n",
    "learning_rate = 0.001\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "timer = Timer()\n",
    "\n",
    "model_RNN = RNN(40, model_AE)\n",
    "\n",
    "loss_function = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_RNN = [tf.keras.metrics.Mean('train_loss'),\n",
    "              tf.keras.metrics.Mean('test_loss')]\n",
    "\n",
    "# loss[0] - train, loss[1] - test\n",
    "losses_RNN = []\n",
    "\n",
    "times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_RNN(model, train_ds, loss_function, optimizer, train_loss_metric):\n",
    "    '''\n",
    "    Training for one epoch. Adjusted for Autoencoder as there are no acc_metric.\n",
    "    '''\n",
    "    for x, label in train_ds:  # there are no (input,label) pairs\n",
    "        with tf.GradientTape() as tape:\n",
    "            # forward pass\n",
    "            prediction = model(x, training=True)\n",
    "            loss = loss_function(x, prediction) \n",
    "\n",
    "        # backward pass\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        train_loss_metric.update_state(loss)\n",
    "\n",
    "# @tf.function\n",
    "# def eval_step_RNN(model, ds, loss_function, loss_metric):\n",
    "#     '''\n",
    "#     Evaluate without training. Adjusted for autoencoder. \n",
    "#     Return a random image and reconstructed version of it.\n",
    "#     '''\n",
    "#     prediction = 0.0\n",
    "#     img = 0.0\n",
    "\n",
    "#     for img, label in ds:\n",
    "#         # forward pass\n",
    "#         prediction = model(img, training=False)\n",
    "        \n",
    "#         # update metrics\n",
    "#         loss = loss_function(img, prediction)\n",
    "#         loss_metric.update_state(loss)\n",
    "        \n",
    "#     return img, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[INFO] - Evaluating the Dataset on the {model_AE.name} before training.')\n",
    "timer.start()\n",
    "\n",
    "# evaluate once before training \n",
    "eval_step_AE(model_AE,\n",
    "             train_ds,\n",
    "             loss_function, \n",
    "             loss_metric=train_loss_metric)\n",
    "\n",
    "eval_step_AE(model_AE,  \n",
    "             test_ds,\n",
    "             loss_function, \n",
    "             loss_metric=test_loss_metric)\n",
    "\n",
    "# Evaluate the metrics\n",
    "eval_metrics(metrics=metrics_RNN, metric_lists=losses_RNN)\n",
    "\n",
    "# Evaluate the timer\n",
    "elapsed_time = timer.stop()\n",
    "times.append(elapsed_time)\n",
    "\n",
    "print(f'train_loss: {train_loss:0.4f}, test_loss: {test_loss:0.4f}')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\n[EPOCH] ____________________{epoch}____________________')\n",
    "    \n",
    "    timer.start()\n",
    "    \n",
    "    # Training step    \n",
    "    train_step_AE(model_AE, train_ds, loss_function, optimizer, train_loss_metric)\n",
    "    \n",
    "    # Test step    \n",
    "    img_original, img_reconstructed = eval_step_AE(model_AE, test_ds, loss_function, test_loss_metric)\n",
    "    \n",
    "    # Evaluate the metrics\n",
    "    eval_metrics(metrics=metrics_RNN, metric_lists=losses_RNN)\n",
    "    \n",
    "    elapsed_time = timer.stop()\n",
    "    times.append(elapsed_time)\n",
    "\n",
    "    print(f'[{epoch}] - Finished Epoch in {elapsed_time:0.2f} seconds - train_loss: {train_loss:0.4f}; test_loss: {test_loss:0.4f}')\n",
    "    \n",
    "    # print progress every while\n",
    "    if epoch%5 == 0:\n",
    "        print(f'\\n[INFO] - Total time elapsed: {np.sum(times)/60:0.4f} min. Total time remaining: {(np.sum(times)/(epoch+1))*(EPOCHS-epoch-1)/60:0.4f} min.')\n",
    "        \n",
    "        # Visualize reconstructed image      \n",
    "        plt.figure(figsize=(9, 3))\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.title('Original Image')\n",
    "        plt.imshow(img_original[0,:,:,0], cmap='gray')\n",
    "\n",
    "        plt.subplot(122)\n",
    "        plt.title('Reconstructed Image')\n",
    "        plt.imshow(img_reconstructed[0,:,:,0], cmap='gray')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "                    \n",
    "print(f'\\n[INFO] - Total run time: {np.sum(times)/60:0.4f} min.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
