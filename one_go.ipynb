{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "one_go.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ82Qp9sZJfH",
        "outputId": "c0f4f6bb-afcf-4dba-aef0-b2fcd34edacb"
      },
      "source": [
        "pip install netCDF4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting netCDF4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/71/a321abeee439caf94850b4f68ecef68d2ad584a5a9566816c051654cff94/netCDF4-1.5.5.1-cp36-cp36m-manylinux2014_x86_64.whl (4.7MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7MB 21.6MB/s \n",
            "\u001b[?25hCollecting cftime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/60/bad8525d2c046eb2964911bc412a85ba240b31c7b43f0c19377233992c6c/cftime-1.3.0-cp36-cp36m-manylinux1_x86_64.whl (295kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.6/dist-packages (from netCDF4) (1.19.5)\n",
            "Installing collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.3.0 netCDF4-1.5.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW3Ba1lc-tp9"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi1l-Mww_NvD",
        "outputId": "57437e31-1d7f-444f-c0fd-cf6ddaae069d"
      },
      "source": [
        "# only when using colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH = '/content/drive/MyDrive/Uni/9/'\n",
        "# PATH =''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSvypfA9BMuI"
      },
      "source": [
        "class Timer():\n",
        "    \"\"\"\n",
        "    A small class to measure time during training.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self._start_time = None\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"\n",
        "        Start a new timer\n",
        "        \"\"\"\n",
        "        self._start_time = time.perf_counter()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"\n",
        "        Stop the timer, and report the elapsed time\n",
        "        \"\"\"\n",
        "        if self._start_time is None:\n",
        "            print(f\"Timer is not running. Use .start() to start it\")\n",
        "            return 0\n",
        "    \n",
        "        elapsed_time = time.perf_counter() - self._start_time\n",
        "        self._start_time = None\n",
        "        \n",
        "        return elapsed_time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s40flIFHYYFX"
      },
      "source": [
        "def load_weights(Model, weight_file, dataset, input_shape):\n",
        "    \"\"\"\n",
        "    Builds the model by using the call method on input and then loads the weights.\n",
        "    \"\"\"\n",
        "    for sequence, t in dataset.take(1):\n",
        "        model2(sequence, training=False, input_shape=input_shape)\n",
        "        \n",
        "    return Model.load_weights()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6zJCdn0-tqD"
      },
      "source": [
        "# Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM8-88j6-tqF"
      },
      "source": [
        "# data param\n",
        "day_len = 30  # actual day length is day_len*stack_days\n",
        "stack_days = 3  # average over stack to allow more day_len\n",
        "batch_size = 16\n",
        "total_months = (2100-2015+1)*12\n",
        "\n",
        "data_models = ['GFDL-ESM4','IPSL-CM6A-LR','MPI-ESM1-2-HR']  # models for temp, prec, LAI\n",
        "dmodel = data_models[1]  # TODO: choose a model randomly\n",
        "scenarios = ['ssp126', 'ssp370', 'ssp585']\n",
        "scenario = scenarios[0]\n",
        "\n",
        "# model param\n",
        "num_filters = 16\n",
        "\n",
        "# training param\n",
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "training_name = \"Version_x\""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IV7Uc8c-tqG"
      },
      "source": [
        "# Training data\n",
        "Ideally, load all data for training and prediction at once, but doesn't fit into memory. Use separate loading scheme for training and deployment instead.\n",
        "\n",
        "naming convention: dx=dataarray, ds=dataset, dn=numpy array. Notice that we save numpy array explictly to save loading time inside generator\n",
        "\n",
        "Use separate data for test and validation set. We use the last 14 years for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0h0aZ1c-tqH",
        "outputId": "b54e5b56-cc7c-4b39-d347-c31461a9210b"
      },
      "source": [
        "# compute average over every stack_days days\n",
        "average_days = lambda dn : np.mean(\n",
        "    np.reshape(dn[:(dn.shape[0]//stack_days)*stack_days],  # cut out left out few days\n",
        "               (-1,stack_days,dn.shape[-2],dn.shape[-1])),\n",
        "               axis=1)\n",
        "\n",
        "# climate historical\n",
        "dn_temp = average_days(np.array(xr.open_mfdataset(PATH+'data/near_surface_air_temperature/historical/{}/*.nc'.format(dmodel)).tas))\n",
        "dn_prec = average_days(np.array(xr.open_mfdataset(PATH+'data/precipitation_flux/historical/{}/*.nc'.format(dmodel)).pr))\n",
        "\n",
        "# vegetation historical\n",
        "npp_files = glob.glob(PATH+'data/net_primary_production_on_land/historical/**/*.nc', recursive=True) # TODO: use all models\n",
        "dn_npp = np.nan_to_num(np.array(xr.open_mfdataset(np.random.choice(np.array(npp_files))).npp)) \n",
        "\n",
        "dx_lai = xr.open_mfdataset(PATH+'data/leaf_area_index/historical/{}/*.nc'.format(dmodel)).lai  # not numpy as time index needed\n",
        "dx_lai = dx_lai.fillna(0)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: FutureWarning: In xarray version 0.15 the default behaviour of `open_mfdataset`\n",
            "will change. To retain the existing behavior, pass\n",
            "combine='nested'. To use future default behavior, pass\n",
            "combine='by_coords'. See\n",
            "http://xarray.pydata.org/en/stable/combining.html#combining-multi\n",
            "\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/xarray/backends/api.py:941: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
            "to use the new `combine_by_coords` function (or the\n",
            "`combine='by_coords'` option to `open_mfdataset`) to order the datasets\n",
            "before concatenation. Alternatively, to continue concatenating based\n",
            "on the order the datasets are supplied in future, please use the new\n",
            "`combine_nested` function (or the `combine='nested'` option to\n",
            "open_mfdataset).\n",
            "  from_openmfds=True,\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: FutureWarning: In xarray version 0.15 the default behaviour of `open_mfdataset`\n",
            "will change. To retain the existing behavior, pass\n",
            "combine='nested'. To use future default behavior, pass\n",
            "combine='by_coords'. See\n",
            "http://xarray.pydata.org/en/stable/combining.html#combining-multi\n",
            "\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.6/dist-packages/xarray/backends/api.py:941: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
            "to use the new `combine_by_coords` function (or the\n",
            "`combine='by_coords'` option to `open_mfdataset`) to order the datasets\n",
            "before concatenation. Alternatively, to continue concatenating based\n",
            "on the order the datasets are supplied in future, please use the new\n",
            "`combine_nested` function (or the `combine='nested'` option to\n",
            "open_mfdataset).\n",
            "  from_openmfds=True,\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: FutureWarning: In xarray version 0.15 the default behaviour of `open_mfdataset`\n",
            "will change. To retain the existing behavior, pass\n",
            "combine='nested'. To use future default behavior, pass\n",
            "combine='by_coords'. See\n",
            "http://xarray.pydata.org/en/stable/combining.html#combining-multi\n",
            "\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/xarray/backends/api.py:941: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
            "to use the new `combine_by_coords` function (or the\n",
            "`combine='by_coords'` option to `open_mfdataset`) to order the datasets\n",
            "before concatenation. Alternatively, to continue concatenating based\n",
            "on the order the datasets are supplied in future, please use the new\n",
            "`combine_nested` function (or the `combine='nested'` option to\n",
            "open_mfdataset).\n",
            "  from_openmfds=True,\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: FutureWarning: In xarray version 0.15 the default behaviour of `open_mfdataset`\n",
            "will change. To retain the existing behavior, pass\n",
            "combine='nested'. To use future default behavior, pass\n",
            "combine='by_coords'. See\n",
            "http://xarray.pydata.org/en/stable/combining.html#combining-multi\n",
            "\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/xarray/backends/api.py:941: FutureWarning: The datasets supplied have global dimension coordinates. You may want\n",
            "to use the new `combine_by_coords` function (or the\n",
            "`combine='by_coords'` option to `open_mfdataset`) to order the datasets\n",
            "before concatenation. Alternatively, to continue concatenating based\n",
            "on the order the datasets are supplied in future, please use the new\n",
            "`combine_nested` function (or the `combine='nested'` option to\n",
            "open_mfdataset).\n",
            "  from_openmfds=True,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9ON7Ik4GCcr"
      },
      "source": [
        "### Input normalization\n",
        "\n",
        "##### Axis\n",
        "---\n",
        "\n",
        "- (x) Consider each location as an independent time series: We assume some regions to have overall higher vegetation than others, this information should be kept\n",
        "\n",
        "- (x) Consider each time slice as an independent image: We assume some seasons to have higher vegetation than others\n",
        "\n",
        "- (o) Normalize over all axis (time, lat, lon)\n",
        "\n",
        "##### Range\n",
        "---\n",
        "- Z-standardize inputs, (0,1)\n",
        "- minmax scale outpus (using tanh)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1V7zBILEKV4"
      },
      "source": [
        "# define train and validation separation\n",
        "train_max_index = dn_temp.shape[0] - (365*14)\n",
        "\n",
        "maxi_in = 10  # used to decide scale of minmax scaling\n",
        "mini_in = 0\n",
        "maxi_out = 10\n",
        "mini_out = 0\n",
        "\n",
        "# Types of normalization\n",
        "z_standard = lambda x : (x - np.mean(x)) / np.std(x)\n",
        "minmax_scale = lambda x, mini_a, maxi_a, min_b, max_b : mini_a + (maxi_a-mini_a) * (x-min_b)/(max_b-min_b)\n",
        "\n",
        "# Input normalization\n",
        "temp_min = np.min(dn_temp[:train_max_index])  # normalize using means only from training data\n",
        "temp_max = np.max(dn_temp[:train_max_index])\n",
        "dn_temp = minmax_scale(dn_temp, mini_in, maxi_in, temp_min, temp_max)\n",
        "\n",
        "prec_min = np.min(dn_prec[:train_max_index])\n",
        "prec_max = np.max(dn_prec[:train_max_index])\n",
        "dn_prec = minmax_scale(dn_prec, mini_in, maxi_in, prec_min, prec_max)\n",
        "\n",
        "# Output normalization, use unshortend index\n",
        "npp_min = np.min(dn_npp[:train_max_index*stack_days])\n",
        "npp_max = np.max(dn_npp[:train_max_index*stack_days])\n",
        "dn_npp = minmax_scale(dn_npp, mini_in, maxi_in, npp_min, npp_max)\n",
        "\n",
        "lai_min = np.min(dx_lai[:train_max_index*stack_days])  \n",
        "lai_max = np.max(dx_lai[:train_max_index*stack_days])\n",
        "dx_lai = minmax_scale(dx_lai, mini_out, maxi_out, lai_min, lai_max)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgkr3aB0F9w8"
      },
      "source": [
        "### Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8SjGcyOGOmn"
      },
      "source": [
        "# range of output month indicies needed for plotting and generator\n",
        "val_months = 12*14 # 14 years of validation data\n",
        "\n",
        "train_max_month = 1978-val_months\n",
        "train_min_month = day_len//28\n",
        "\n",
        "val_max_month = 1978\n",
        "val_min_month = train_max_month +1"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vl13LRaYYFl"
      },
      "source": [
        "def train_gen_data_card():\n",
        "    while True:\n",
        "        # array to append to\n",
        "        endstamp = []\n",
        "        output_day_i = np.zeros(batch_size)\n",
        "        lai = np.zeros((batch_size, dn_npp.shape[1], dn_npp.shape[2]))  # batch, lon, lat\n",
        "        npp = np.zeros((batch_size, dn_npp.shape[1], dn_npp.shape[2]))\n",
        "        temp = np.zeros((batch_size, day_len, dn_npp.shape[1], dn_npp.shape[2]))  # batch, time, lon, lat\n",
        "        prec = np.zeros((batch_size, day_len, dn_npp.shape[1], dn_npp.shape[2]))\n",
        "        \n",
        "        # index of output in month\n",
        "        output_month_i = np.random.randint(train_min_month, train_max_month, size=batch_size)  # y_pred timepoint in int\n",
        "\n",
        "        # convert output index to timestamp\n",
        "        try:\n",
        "            for i in range(batch_size):\n",
        "                endstamp.append(dx_lai.indexes['time'].to_datetimeindex()[output_month_i[i]])  # cfttimeindex to datetime               \n",
        "        except:\n",
        "            for i in range(batch_size):\n",
        "                endstamp.append(dx_lai.indexes['time'][output_month_i[i]])\n",
        "\n",
        "        # convert output month index to day index\n",
        "        for i in range(batch_size):\n",
        "            output_day_i[i] = (endstamp[i] - pd.Timestamp('1850-01-01T12')).days  # output is i-th day in int\n",
        "        output_day_i = np.int_(output_day_i) // stack_days  # 0,1,2 is stacked onto 0, 3,4,5 to 1 etc.\n",
        "\n",
        "        # save month-based time slices\n",
        "        lainp = np.array(dx_lai)\n",
        "        for i in range(batch_size):\n",
        "            lai[i] = lainp[output_month_i[i]]\n",
        "        for i in range(batch_size):\n",
        "            npp[i] = dn_npp[output_month_i[i]]\n",
        "\n",
        "        # day-based metrics\n",
        "        for i in range(batch_size):\n",
        "            temp[i] = dn_temp[output_day_i[i]-day_len:output_day_i[i]]\n",
        "        for i in range(batch_size):\n",
        "            prec[i] = dn_prec[output_day_i[i]-day_len:output_day_i[i]]\n",
        "\n",
        "        # merge features\n",
        "        inputs = np.stack((temp,prec), axis=-1)  # two features\n",
        "        outputs = np.stack((lai,npp), axis=-1)\n",
        "\n",
        "        yield (inputs, outputs)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vFmFJVnYYFl"
      },
      "source": [
        "def val_gen_data_card():\n",
        "    while True:\n",
        "        # array to append to\n",
        "        endstamp = []\n",
        "        output_day_i = np.zeros(batch_size)\n",
        "        \n",
        "        lai = np.zeros((batch_size, dn_npp.shape[1], dn_npp.shape[2]))  # batch, lon, lat\n",
        "        npp = np.zeros((batch_size, dn_npp.shape[1], dn_npp.shape[2]))\n",
        "        temp = np.zeros((batch_size, day_len, dn_npp.shape[1], dn_npp.shape[2]))  # batch, time, lon, lat\n",
        "        prec = np.zeros((batch_size, day_len, dn_npp.shape[1], dn_npp.shape[2]))\n",
        "        \n",
        "        # index of output in month\n",
        "        output_month_i = np.random.randint(val_min_month, val_max_month, size=batch_size)  # y_pred timepoint in int\n",
        "\n",
        "        # convert output index to timestamp\n",
        "        try:\n",
        "            for i in range(batch_size):\n",
        "                endstamp.append(dx_lai.indexes['time'].to_datetimeindex()[output_month_i[i]])  # cfttimeindex to datetime               \n",
        "        except:\n",
        "            for i in range(batch_size):\n",
        "                endstamp.append(dx_lai.indexes['time'][output_month_i[i]])\n",
        "\n",
        "        # convert output month index to day index\n",
        "        for i in range(batch_size):\n",
        "            output_day_i[i] = (endstamp[i] - pd.Timestamp('1850-01-01T12')).days  # output is i-th day in int\n",
        "        output_day_i = np.int_(output_day_i)\n",
        "\n",
        "        # save month-based time slices\n",
        "        lainp = np.array(dx_lai)\n",
        "        for i in range(batch_size):\n",
        "            lai[i] = lainp[output_month_i[i]]\n",
        "        for i in range(batch_size):\n",
        "            npp[i] = dn_npp[output_month_i[i]]\n",
        "\n",
        "        # day-based metrics\n",
        "        for i in range(batch_size):\n",
        "            temp[i] = dn_temp[output_day_i[i]-day_len:output_day_i[i]]\n",
        "        for i in range(batch_size):\n",
        "            prec[i] = dn_prec[output_day_i[i]-day_len:output_day_i[i]]\n",
        "\n",
        "        # merge features\n",
        "        inputs = np.stack((temp,prec), axis=-1)  # two features\n",
        "        outputs = np.stack((lai,npp), axis=-1)\n",
        "\n",
        "        yield (inputs, outputs)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z20pTYSJYYFl"
      },
      "source": [
        "### tf.Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W2MBk6gYYFn"
      },
      "source": [
        "# Create tf.Dataset objects\n",
        "train_ds = tf.data.Dataset.from_generator(train_gen_data_card, output_types = (tf.float32,tf.float32))\n",
        "train_ds = train_ds.take(30).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_generator(val_gen_data_card, output_types = (tf.float32,tf.float32))\n",
        "val_ds = val_ds.take(30).prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45pk4mrlYYFo"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLSzNzv4YYFo"
      },
      "source": [
        "class ConvLSTM(tf.keras.Model):\n",
        "    def __init__(self, num_filters):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "        \n",
        "        do = 0\n",
        "        self.convlstm2D_1 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
        "                                                     padding=\"same\", return_sequences=True,\n",
        "                                                      activation = \"tanh\",dropout= do)\n",
        "        self.bn_1 = tf.keras.layers.BatchNormalization()\n",
        "        #self.acti_1 = tf.keras.layers.Activation(activation)\n",
        "        \n",
        "        self.convlstm2D_2 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
        "                                                     padding=\"same\", return_sequences=True,\n",
        "                                                      activation = \"tanh\",dropout=do)\n",
        "        self.bn_2 = tf.keras.layers.BatchNormalization()\n",
        "        #self.acti_2 = tf.keras.layers.Activation(activation)\n",
        "\n",
        "\n",
        "        self.convlstm2D_3 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
        "                                                     padding=\"same\", return_sequences=True,\n",
        "                                                      activation = \"tanh\",dropout=do)\n",
        "        self.bn_3 = tf.keras.layers.BatchNormalization()\n",
        "        #self.acti_3 = tf.keras.layers.Activation(activation)\n",
        "        \n",
        "        self.convlstm2D_4 = tf.keras.layers.ConvLSTM2D(filters = num_filters, kernel_size=(3,3),\n",
        "                                                     padding=\"same\", return_sequences=True,\n",
        "                                                      activation = \"tanh\",dropout=do)\n",
        "        self.bn_4 = tf.keras.layers.BatchNormalization()\n",
        "        #self.acti_4 = tf.keras.layers.Activation(activation)\n",
        "        \n",
        "        # convolve over time, lat, lon. This means that we assume timesteps close to each other share local similarities\n",
        "        self.conv3d = tf.keras.layers.Conv3D(filters = 2, kernel_size = (3,3,3), \n",
        "                                             activation= \"tanh\", padding=\"same\")\n",
        "        \n",
        "        self.do3d = tf.keras.layers.Dropout(do)\n",
        "        # computed convolved sum over all time dimension to get a single time slice\n",
        "        self.bottleneck = tf.keras.layers.Conv3D(filters=1, kernel_size=1, activation=\"relu\",strides=1)\n",
        "\n",
        "\n",
        "    def call(self, x, training, input_shape):\n",
        "        # (batch, time, lat, lon, channel)\n",
        "        x = tf.ensure_shape(x, input_shape) \n",
        "        # (batch, time, lat, lon, channel)\n",
        "        x = self.convlstm2D_1(x,training= training)\n",
        "        # (batch, time, lat1, lon1, filter1)\n",
        "        x = self.bn_1(x,training = training)\n",
        "        \n",
        "        x = self.convlstm2D_2(x,training = training)\n",
        "        x = self.bn_2(x,training = training)\n",
        "        \n",
        "        x = self.convlstm2D_3(x,training = training)\n",
        "        x = self.bn_3(x,training = training)\n",
        "        \n",
        "        x = self.convlstm2D_4(x,training = training)\n",
        "        x = self.bn_4(x, training = training)\n",
        "        # (batch, time, lat4, lon4, filter4)\n",
        "        x = self.do3d(x,training= training)\n",
        "        x = self.conv3d(x)\n",
        "        # (batch, newtime, newlat, newlon, newfilter=2)\n",
        "        \n",
        "        x = tf.transpose(x, [0,4,2,3,1])\n",
        "        # (batch, 2, lat, lon, time)\n",
        "        x = self.bottleneck(x)\n",
        "        # (batch, 2, lat, lon, 1)\n",
        "        \n",
        "        x = tf.transpose(x, [0,4,2,3,1])\n",
        "        # (batch, 1, lat, lon, 2)\n",
        "        x = tf.squeeze(x,axis=1)\n",
        "        # (batch, lat, lon, 2)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz0NLhSpYYFp"
      },
      "source": [
        "model = ConvLSTM(num_filters=num_filters)\n",
        "input_shape = (batch_size, day_len, 36, 72, 2)\n",
        "# model.build(input_shape)\n",
        "# model.summary() # TODO: doesn't work cuz some layers aren't built?\n",
        "# TODO: The fact that we can't build the model probably results in undefined rank error"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gXTFFmGYYFq"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS80mihOYYFr"
      },
      "source": [
        "@tf.function\n",
        "def train_step(model, data, loss_function, optimizer, train_loss_metric, input_shape=input_shape):\n",
        "    '''\n",
        "    Training for one epoch.\n",
        "    '''\n",
        "    for img, target in train_ds:\n",
        "        # forward pass with GradientTape\n",
        "        with tf.GradientTape() as tape:\n",
        "            prediction = model(img, training=True, input_shape=input_shape)\n",
        "            loss = loss_function(target, prediction) + tf.reduce_sum(model.losses)\n",
        "            # TODO: does model have reg loss if we don't use kernel reg?\n",
        "\n",
        "        # backward pass via GradienTape (auto-gradient calc)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        # update metrics\n",
        "        train_loss_metric.update_state(loss)\n",
        "        \n",
        "#@tf.function        \n",
        "def eval_step(model, ds, loss_function, loss_metric, input_shape=input_shape):\n",
        "    '''\n",
        "    Evaluation Loop.\n",
        "    '''\n",
        "    for sequence, target in ds:\n",
        "        # forward pass\n",
        "        prediction = model(sequence, training=False, input_shape=input_shape)\n",
        "        # update metrics\n",
        "        loss = loss_function(target, prediction)\n",
        "        loss_metric.update_state(loss)\n",
        "        \n",
        "    fig, axe = plt.subplots(2,3, figsize=(25,10))\n",
        "    plt.tight_layout(pad= 0.05)\n",
        "    axe[0,0].imshow(target[0,:,:,0], cmap='gray', origin='lower')\n",
        "    axe[0,1].imshow(prediction[0,:,:,0], cmap='gray', origin='lower')\n",
        "    axe[1,0].imshow(target[0,:,:,1], cmap='gray', origin='lower')\n",
        "    axe[1,1].imshow(prediction[0,:,:,1], cmap='gray', origin='lower')\n",
        "    axe[0,2].imshow(np.abs(target[0,:,:,0]-prediction[0,:,:,0]), cmap = \"bwr\", \n",
        "                    origin= \"lower\", vmin = 0,vmax =maxi_out)\n",
        "    axe[1,2].imshow(np.abs(target[0,:,:,1]-prediction[0,:,:,1]), cmap = \"bwr\", \n",
        "                    origin= \"lower\", vmin = 0,vmax =maxi_out)\n",
        "\n",
        "\n",
        "# TODO: maybe we want another function (predict) to deploy the model to predict future vegetation and automatically save?"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml7mlt3qYYFr"
      },
      "source": [
        "timer = Timer()\n",
        "\n",
        "# define loss\n",
        "loss_function = tf.keras.losses.MSE\n",
        "# define optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "# 2nd-order metric to take mean over all samples\n",
        "train_loss_metric = tf.keras.metrics.Mean('train_loss')\n",
        "val_loss_metric = tf.keras.metrics.Mean('val_loss')\n",
        "\n",
        "# initialize the logger for Tensorboard visualization\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train_ConvLSTM'    \n",
        "val_log_dir = 'logs/gradient_tape/' + current_time + '/val_ConvLSTM'       \n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)  \n",
        "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
        "\n",
        "times = []"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r3X0RFGjYYFr",
        "outputId": "de78aca3-ba3e-4b7a-eae2-8e7bd45ab3e8"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'\\n[EPOCH] ____________________{epoch}____________________')\n",
        "    \n",
        "    # training step with metrics update--------------------------------------------------------\n",
        "    timer.start()\n",
        "    train_step(model, train_ds, loss_function, optimizer, train_loss_metric)\n",
        "    train_loss = train_loss_metric.result()\n",
        "    with train_summary_writer.as_default():     # logging our metrics to a file which is used by tensorboard\n",
        "        tf.summary.scalar('loss', train_loss, step=epoch)\n",
        "    elapsed_time = timer.stop()\n",
        "    print(f'[{epoch}] - Finished Epoch in {elapsed_time:0.2f} seconds - train_loss: {train_loss:0.4f}')\n",
        "    \n",
        "    # evaluation step with metrics update--------------------------------------------------------\n",
        "    timer.start()\n",
        "    eval_step(model, val_ds, loss_function, loss_metric=val_loss_metric)\n",
        "    plt.show()\n",
        "    val_loss = val_loss_metric.result()    \n",
        "    with val_summary_writer.as_default():       # logging our metrics to a file which is used by tensorboard\n",
        "        tf.summary.scalar('loss', val_loss, step=epoch)    \n",
        "    print(f'\\n[{epoch}] - Finished evaluation - val_loss: {val_loss:0.4f}')#, val_accuracy: {val_acc:0.4f}')\n",
        "    \n",
        "    # Resetting train and validation metrics-----------------------------------------------------\n",
        "    train_loss_metric.reset_states()\n",
        "    val_loss_metric.reset_states()\n",
        "    elapsed_time = timer.stop()\n",
        "    times.append(elapsed_time)\n",
        "  \n",
        "    if epoch%3 == 0:\n",
        "        print(f'\\n[INFO] - Total time elapsed: {np.sum(times)/60:0.4f} min. Total time remaining: {(np.sum(times)/(epoch+1))*(epochs-epoch-1)/60:0.4f} min.')\n",
        "\n",
        "print(f'[INFO] - Total run time: {np.sum(times)/60:0.4f} min.')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[EPOCH] ____________________0____________________\n",
            "[0] - Finished Epoch in 38.44 seconds - train_loss: 1.1600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2112\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2113\u001b[0;31m       \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2578\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2579\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2580\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: ValueError: could not broadcast input array from shape (0,36,72) into shape (30,36,72)\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 891, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-25-077ec58a1b66>\", line 37, in val_gen_data_card\n    temp[i] = dn_temp[output_day_i[i]-day_len:output_day_i[i]]\n\nValueError: could not broadcast input array from shape (0,36,72) into shape (30,36,72)\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-382c741193b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# evaluation step with metrics update--------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loss_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-add967903b03>\u001b[0m in \u001b[0;36meval_step\u001b[0;34m(model, ds, loss_function, loss_metric, input_shape)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mEvaluation\u001b[0m \u001b[0mLoop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     '''\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2116\u001b[0;31m       \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: ValueError: could not broadcast input array from shape (0,36,72) into shape (30,36,72)\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py\", line 620, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 891, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-25-077ec58a1b66>\", line 37, in val_gen_data_card\n    temp[i] = dn_temp[output_day_i[i]-day_len:output_day_i[i]]\n\nValueError: could not broadcast input array from shape (0,36,72) into shape (30,36,72)\n\n\n\t [[{{node PyFunc}}]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5VUSjnjYYFs"
      },
      "source": [
        "# save weights of model\n",
        "model.save_weights(\"model_weights/\"+ training_name + \".hdf5\",save_format=\"hdf5\",overwrite=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8J_FNFK-tqI"
      },
      "source": [
        "# Prediction data\n",
        "one could also use just function. Systematically sliding window to predict all future months using timestamp index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqgpmrLuYYFt"
      },
      "source": [
        "# climate historical\n",
        "dx_temp = xr.open_mfdataset(PATH+'data/near_surface_air_temperature/historical/{}/*.nc'.format(dmodel)).tas\n",
        "dx_prec = xr.open_mfdataset(PATH+'data/precipitation_flux/historical/{}/*.nc'.format(dmodel)).pr\n",
        "\n",
        "# climate future\n",
        "dx_temp_future = xr.open_mfdataset(PATH+'data/near_surface_air_temperature/{}/{}/*.nc'.format(scenario, dmodel)).tas\n",
        "dx_temp_future.tas = minmax_scale(dx_temp_future.tas, mini_in, maxi_in, temp_min, temp_max)  # scale with training data minmax\n",
        "dx_temp_future = xr.concat((dx_temp, dx_temp_future), dim='time')\n",
        "dx_prec_future = xr.open_mfdataset(PATH+'data/precipitation_flux/{}/{}/*.nc'.format(scenario, dmodel)).pr\n",
        "dx_temp_future.pr = minmax_scale(dx_temp_future.pr, mini_in, maxi_in, prec_min, prec_max)\n",
        "dx_prec_future = xr.concat((dx_prec, dx_prec_future), dim='time')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sfgGGc--tqJ"
      },
      "source": [
        "def gen_future_climate():\n",
        "#     first_month = (2015-1850)*12 + 1  # int index of which month january 2015 is with 0 being january 1850\n",
        "\n",
        "    counter = 0\n",
        "    while counter < total_months: # predict from 2015 to 2100, excluding last month\n",
        "        cyear = (counter//12) + 2015\n",
        "        \n",
        "        cmonth = (counter+1) % 12\n",
        "        \n",
        "        if cmonth == 0:\n",
        "            cmonth = 12\n",
        "        \n",
        "        current_timestamp = pd.Timestamp(cyear, cmonth, 1)\n",
        "        input_start_timestamp = current_timestamp - pd.Timedelta(day_len-1, unit='day')\n",
        "\n",
        "        counter += 1\n",
        "       \n",
        "        data = np.stack((np.array(dx_temp_future.loc[input_start_timestamp:current_timestamp+pd.Timedelta(1,unit='day')]),\n",
        "                         np.array(dx_prec_future.loc[input_start_timestamp:current_timestamp+pd.Timedelta(1,unit='day')])),\n",
        "                         axis=-1)\n",
        "        \n",
        "        yield data\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAod8bCF-tqL"
      },
      "source": [
        "# data pipeline\n",
        "ds_future = tf.data.Dataset.from_generator(generator=gen_future_climate, \n",
        "                                           output_types=(tf.float32)).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22u7zmaL-tqL",
        "scrolled": true
      },
      "source": [
        "for i in ds_future.take(1):\n",
        "    print(i.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFpUwC1S-tqR"
      },
      "source": [
        "# Predict future"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wMFN7t_YYFx"
      },
      "source": [
        "# Load weights if using separately from training\n",
        "# model = load_weights(PFAD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UtB78pk-tqT",
        "scrolled": true
      },
      "source": [
        "for data in ds_future:  # just batch whole data into one batch\n",
        "    try:\n",
        "        predictions = np.concatenate((predictions, model(data, training=False, input_shape=input_shape)), axis=0)    \n",
        "    except Exception as e:\n",
        "        predictions = model(data, training=False, input_shape=input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-T1BGKZ-tqU"
      },
      "source": [
        "np.save('results/pred_{}_{}'.format(training_name,scenario), predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ATcbPZo-tql"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgHjdLx9-tqn"
      },
      "source": [
        "plt.rcParams[\"animation.html\"] = \"jshtml\"  # allow animation for jupyter\n",
        "plt.rcParams['xtick.bottom'] = False\n",
        "plt.rcParams['xtick.labelbottom'] = False\n",
        "plt.rcParams['ytick.left'] = False\n",
        "plt.rcParams['ytick.labelleft'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOQIY7xU-tqn"
      },
      "source": [
        "print(predictions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUYP5ON9-tqn"
      },
      "source": [
        "frames = []  # append each image\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)\n",
        "ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
        "ax.set_axis_off()\n",
        "fig.add_axes(ax)\n",
        "\n",
        "for timeindex in range(total_months):  # animate for 1 yr\n",
        "\n",
        "    frames.append([plt.imshow(predictions[timeindex,:,:,0], # TODO: change to 1 to save NPP\n",
        "                              cmap='gray', origin='lower', animated=True)])\n",
        "\n",
        "ani = matplotlib.animation.ArtistAnimation(fig, frames, interval=100, blit=True, repeat=True)\n",
        "# ani.save('figs/pred_v1_{}_LAI.gif'.format(scenario), writer='imagemagick', fps=60)\n",
        "ani\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}